[
    {
        "content": "Yes, it is because you use 4kb granularity in the GDT entry. From the [Intel&#174; 64 and IA-32 Architectures Developer&#39;s Manual: Vol. 3A][1] regarding _5.3 Limit Checking_:\r\n\r\n&gt; When the G flag is clear (byte granularity), the effective limit is the value of the 20-bit limit field in the segment\r\ndescriptor. Here, the limit ranges from 0 to FFFFFH (1 MByte). **When the G flag is set (4-KByte page granularity),\r\nthe processor scales the value in the limit field by a factor of 2^12 (4 KBytes)**. In this case, the effective limit ranges\r\nfrom FFFH (4 KBytes) to FFFFFFFFH (4 GBytes). Note that when scaling is used (G flag is set), the lower 12 bits of\r\na segment offset (address) are not checked against the limit; for example, note that if the segment limit is 0,\r\noffsets 0 through FFFH are still valid.\r\n\r\nFrom the [Intel&#174; 64 and IA-32 Architectures Developer&#39;s Manual: Vol.",
        "score": 25.328125,
        "rank": 1,
        "document_id": "d5ce05ea-8dfa-4d84-9a56-68f7d47123b0",
        "passage_id": 240129
    },
    {
        "content": "From Hans Passant comments and additional tests on my side, here is a sounder answer:\r\n\r\nThe 15.6ms (1/64 second) limit is [well-known][2] on Windows and is the default behavior. It is possible to lower the limit (e.g. to 1ms, through a  call to `timeBeginPeriod()`) though we are not advise to do so, because this affects the global system timer resolution and the resulting [power consumption][3]. For instance, [Chrome is notorious for doing this\u200c\u200b][4]. Hence, due to the global aspect of the timer resolution, one may observe a 1ms precision without explicitly asking for, because of third party programs.\r\n\r\nBesides, be aware that `std::chrono::high_resolution_clock` does **not** have a valid behavior on windows (both in [Visual Studio][1] or MinGW context). So you cannot expect this interface to be a cross-platform solution, and the 15.625ms limit still applies.\r\n\r\n\r\nKnowing that, how can we deal with it?",
        "score": 25.25,
        "rank": 2,
        "document_id": "1b6983c7-0660-4de5-90cc-879809a6c571",
        "passage_id": 84894
    },
    {
        "content": "It reports time with the millisecond precision, but the granularity is not guaranteed.  In fact, if there is no processes that requested higher granularity, you will get up to 55ms accuracy.  This is most likely what&#39;s happening with your code.  here is a quick sample that allows you to figure out the current GetSystemTime() granularity:\r\n\r\n    SYSTEMTIME t1, t2;\r\n    GetSystemTime(&amp;t1);\r\n    do {\r\n        GetSystemTime(&amp;t2);\r\n    } while (0 == memcmp(&amp;t1, &amp;t2, sizeof(SYSTEMTIME)));\r\n    std::cout &lt;&lt; &quot;timer granularity is &quot; &lt;&lt; t2.wMilliseconds - t1.wMilliseconds &lt;&lt; &quot; milliseconds&quot; &lt;&lt; std::endl;\r\n\r\nAnswering your original question, you should see at least ~700MBps System-&gt;video memory transfer, even on the very slow kinds of old PCIe v.1.0 cards.",
        "score": 25.0625,
        "rank": 3,
        "document_id": "644f0369-4578-4282-9afc-b6b44826bc33",
        "passage_id": 412871
    },
    {
        "content": "You cannot measure to 1 nanosecond, you cannot measure to 10 nanosconds either. This is because each `action of measurement` requires a call of some kind. One of the fastest APIs is \r\nGetSystemTimeAsFileTime(). A call requires 10-15ns. But it&#39;s resolution/granularity is rather poor (in the ms regime). QueryPerformanceCounter() delivers frequencies in the MHz to GHz range, depending on the underlaying hardware. This call is not as fast but at 1MHz you get 1 microsecond resolution. At such a frequency, given by QueryPerformanceFrequency(), consecutive call will may return equal values because the call is faster than the increment rate. \r\nAnother source is the CPU time stamp counter (rdtsc). But there are some drawback with it too: Modern hardware implements adpative CPU frequency. Therefore this frequency cannot be considered as a constant. This way measurements are only posible during constant phases.\r\n\r\nIn fact none of the frequency sources delivers a constant frequency. All of these frequencies are generated by some hardware which has `offset and drift`.",
        "score": 24.953125,
        "rank": 4,
        "document_id": "4301882c-1d65-4e17-9e5a-8ec57048a83c",
        "passage_id": 102048
    },
    {
        "content": "So, having memory tracing enabled (more than 10% or mem.access tracing) clearly will limit available memory bandwidth and the program will run slower. Even 1% tracing can be noted, but it effect (overhead) is smaller.\r\n\r\nYour  CPU E5-2620 v4 is Broadwell-EP 14nm so it may have also some earlier variant of the Intel PT: https://software.intel.com/en-us/blogs/2013/09/18/processor-tracing https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/intel-pt.txt https://github.com/01org/processor-trace and especially Andi Kleen&#39;s blog on [pt](http://halobates.de/blog/p/category/pt): http://halobates.de/blog/p/410 &quot;Cheat sheet for Intel Processor Trace with Linux perf and gdb&quot;\r\n\r\n&gt; PT support in hardware: Broadwell (5th generation Core, Xeon v4)\tMore overhead. No fine grained timing.",
        "score": 24.75,
        "rank": 5,
        "document_id": "49df2682-33ed-42d6-8993-472bad0685e3",
        "passage_id": 38148
    },
    {
        "content": "Such instrumentation is from 2x slow or slower, especially for memory bandwidth / latency limited core. The paper lists and explains instrumentation overhead and Caveats:\r\n\r\n&gt; **Instrumentation Overhead**: Instrumentation involves\r\ninjecting extra code dynamically or statically into the\r\ntarget application. The additional code causes an\r\napplication to spend extra time in executing the original\r\napplication ... Additionally, for multi-threaded\r\napplications, instrumentation can modify the ordering of\r\ninstructions executed between different threads of the\r\napplication. As a result, IDS with multi-threaded\r\napplications comes at the lack of some fidelity\r\n&gt;\r\n&gt; **Lack of Speculation**: Instrumentation only observes\r\ninstructions executed on the correct path of execution. As\r\na result, IDS may not be able to support wrong-path ...\r\n&gt;\r\n&gt; **User-level Traffic Only**: Current binary instrumentation\r\ntools only support user-level instrumentation. Thus,\r\napplications that are kernel intensive are unsuitable for\r\nuser-level IDS. \r\n\r\n\r\n  [1]: https://software.intel.com/en-us/articles/intel-sdm",
        "score": 24.59375,
        "rank": 6,
        "document_id": "49df2682-33ed-42d6-8993-472bad0685e3",
        "passage_id": 38150
    },
    {
        "content": "(Hardware Lock Elision is a very constrained implementation of silent ABA store elimination. It has the implementation advantage that the check for value consistency is explicitly requested.)\r\n\r\nThere are also implementation issues in terms of performance impact/design complexity. Such would prohibit avoiding read-for-ownership (unless the silent store elimination was only active when the cache line was already present in shared state), though read-for-ownership avoidance is also currently not implemented.\r\n\r\nSpecial handling for silent stores would also complicate implementation of a memory consistency model (probably especially x86&#39;s relatively strong model). Such might also increase the frequency of rollbacks on speculation that failed consistency. If silent stores were only supported for L1-present lines, the time window would be very small and rollbacks *extremely* rare; stores to cache lines in L3 or memory might increase the frequency to very rare, which might make it a noticeable issue.\r\n\r\nSilence at cache line granularity is also less common than silence at the access level, so the number of invalidations avoided would be smaller.\r\n\r\nThe additional cache bandwidth would also be an issue. Currently Intel uses parity only on L1 caches to avoid the need for read-modify-write on small writes.",
        "score": 24.0625,
        "rank": 7,
        "document_id": "590a08d8-994d-4b86-8912-115df40b5131",
        "passage_id": 46992
    },
    {
        "content": "The conditions describing the limits for concurrent kernel execution are given in the CUDA programming guide ([link][1]), but the gist of it is that your GPU can potentially run multiple kernels from different streams only if your GPU has sufficient resources to do so.\r\n\r\nIn your usage case, you have said that you are running multiple launches of a kernel with 150 blocks of 512 threads each. Your GPU has 12 SMM (I think), and you could have *up to* 4 blocks per SMM running concurrently (4 * 512 = 2048 threads, which is the SMM limit). So your GPU can only run a maximum of 4 * 12 = 48 blocks concurrently. When multiple launches of 150 blocks sitting in the command pipeline, it would seem that there is little (perhaps even no) opportunity for concurrent kernel execution.\r\n\r\nYou *might* be able to encourage kernel execution overlap if you increase the scheduling granularity of you kernel by reducing the block size. Smaller blocks are more likely to find available resources and scheduling slots than larger blocks.",
        "score": 24.046875,
        "rank": 8,
        "document_id": "3549224a-749a-493b-853a-eeef8194657a",
        "passage_id": 20650
    },
    {
        "content": "This is the first time I hear about AVX2. But I&#39;m guessing the memory alignment restriction won&#39;t be different from current implementation of AVX on Sandy Bridge with the new VEX coding scheme. I.e. no alignment required unless explicitly using aligned `VMOV` instruction with `A` in the name. Most instruction allow access with any byte-granularity alignment. \r\n\r\nIn fact, see section 2.5, page 35 of [Intel(R) Advanced Vector Extensions Programming Reference][1] which states exactly this.\r\n\r\n\r\n  [1]: http://software.intel.com/file/36945",
        "score": 23.890625,
        "rank": 9,
        "document_id": "0be2fc97-7f4e-4022-a193-36b7753c3554",
        "passage_id": 465225
    },
    {
        "content": "I&#39;d suggest you try looking at top, oprofile, or other tools to determine what is going on with the machine that is taking the time. top can also help you determine whether RAM or CPU is the issue, and can provide much more granularity than the graph you&#39;re showing from the GCP web console. (You could also try out Stackdriver in the Basic tier to get more detail on the resource utilization, which might help you figure out the spikes).\r\n\r\nOne note - you say you&#39;re using an n1-standard-1 with 2 CPUs and 3.75GB RAM, but that is not a combination we have. An n1-standard-1 would have 1 VCPU and 3.75, and an n1-standard 2 would have 2CPU-7.5GB.\r\n\r\nAn option to see if machine size is the limitation would be to power down the VM, change the size to something big like an n1-standard-32, restart, and see if it goes faster.\r\n\r\nAnother thing to investigate would be whether you are limited by disk performance. Note that our PD (boot disk) performance is related to the *overall* size of the disk.",
        "score": 23.84375,
        "rank": 10,
        "document_id": "75d7de60-6d05-4bcd-b779-a131cc5ffbe1",
        "passage_id": 312742
    },
    {
        "content": "Silence at cache line granularity is also less common than silence at the access level, so the number of invalidations avoided would be smaller.\r\n\r\nThe additional cache bandwidth would also be an issue. Currently Intel uses parity only on L1 caches to avoid the need for read-modify-write on small writes. Requiring *every* write to have a read in order to detect silent stores would have obvious performance and power implications. (Such reads \r\ncould be limited to shared cache lines and be performed opportunistically, exploiting cycles without full cache access utilization, but that would still have a power cost.) This also means that this cost would fall out if read-modify-write support was already present for L1 ECC support (which feature would please some users).\r\n\r\nI am not well-read on silent store elimination, so there are probably other issues (and workarounds). \r\n\r\nWith much of the low-hanging fruit for performance improvement having been taken, more difficult, less beneficial, and less general optimizations become more attractive. Since silent store optimization becomes more important with higher inter-core communication and inter-core communication will increase as more cores are utilized to work on a single task, the value of such seems likely to increase.",
        "score": 23.828125,
        "rank": 11,
        "document_id": "590a08d8-994d-4b86-8912-115df40b5131",
        "passage_id": 46993
    },
    {
        "content": "Warp Allocation Granularity is a constraint in the hardware resource allocation.\r\n\r\nOn SM1.x-2.x resources are allocated 2 warps at a time.\r\n\r\nOn SM3.x-5.x resources are allocated 4 warps at a time.\r\n\r\nIf the kernel configuration is N then the hardware allocates resources for N rounded up to a multiple of WarpAllocationGranularity.\r\n\r\nThis limitation reduces the control logic and allocation table sizes thus reducing area and power.",
        "score": 23.8125,
        "rank": 12,
        "document_id": "57af1378-76ab-46a8-a7a2-3861ee11ded1",
        "passage_id": 24295
    },
    {
        "content": "Why you want no overhead and all memory accesses recorded? It is just impossible as every memory access have tracing of several bytes (the memory address, sometimes: instruction address) to be recorded to the same memory. So, having memory tracing enabled (more than 10% or memory access tracing) clearly will limit available memory bandwidth and the program will run slower. Even 1% tracing can be noted, but it effect (overhead) is smaller.\r\n\r\nYour  CPU E5-2620 v4 is Broadwell-EP 14nm so it may have also some earlier variant of the Intel PT: https://software.intel.com/en-us/blogs/2013/09/18/processor-tracing https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/intel-pt.txt https://github.com/01org/processor-trace and especially Andi Kleen&#39;s blog on [pt](http://halobates.de/blog/p/category/pt): http://halobates.de/blog/p/410 &quot;Cheat sheet for Intel Processor Trace with Linux perf and gdb&quot;\r\n\r\n&gt; PT support in hardware: Broadwell (5th generation Core, Xeon v4)\tMore overhead.",
        "score": 23.796875,
        "rank": 13,
        "document_id": "99e9edab-d2a4-4aa3-8241-1cd37ee18d47",
        "passage_id": 296373
    },
    {
        "content": "When you scale up the number of threads for MKL/*gemm, consider\r\n\r\n - Memory /shared cache bandwidth may become a bottleneck, limiting the scalability\r\n - Turbo mode will effectively decrease the core frequency when increasing utilization. This applies even when you run at nominal frequency: On Haswell-EP processors, AVX instructions will impose a lower &quot;AVX base frequency&quot; - but the processor is allowed to exceed that when less cores are utilized / thermal headroom is available and in general even more for a short time. If you want perfectly neutral results, you would have to use the AVX base frequency, which is 1.9 GHz for you. It is documented [here][2], and explained in [one picture][3].\r\n\r\nI don&#39;t think there is a really simple way to measure how your application is affected by bad scheduling. You can expose this with `perf trace -e sched:sched_switch` and there is [some software][4] to visualize this, but this will come with a high learning curve. And then again - for parallel performance analysis you should have the threads pinned anyway.",
        "score": 23.671875,
        "rank": 14,
        "document_id": "4f0504f0-2eb5-4334-bf3f-4e6d7074820e",
        "passage_id": 80273
    },
    {
        "content": "Building such a software floating point system is entirely viable and relatively well explored. However I am not aware of a Java/Clojure implementation thereof.\r\n\r\nTrue arbitrary precision is not possible because we have finite memory machines to build upon, therefore at some point we must compromise on performance, memory and precision. Given some library which can correctly and generally represent an arbitrary taylor series evaluation as a sequence of decimal digits at some point, I claim that the overwhemling majority of operations on such arbitrary numbers will be truncated to some precision P either due to the need to perform comparison against a fixed precision representation such as a float or double because they are the industry standards for floating point representation.\r\n\r\nTo blow this well and truly out of the water, at a distance of 1 light-year an angular deviation of 1e-100 degrees would result in a navigational error of approximately 1.65117369558e-86 meteres. This means that the existing machine epsilon of 5x10e-16 with IEEE DOUBLE/64 is entirely acceptable even for interstellar navigation.",
        "score": 23.65625,
        "rank": 15,
        "document_id": "94aa771e-3930-4c71-9531-35fb4505e580",
        "passage_id": 100220
    },
    {
        "content": "The Fortran 2008 Standard defines a [maximum rank of 15](http://fortranwiki.org/fortran/show/Fortran+2008). `ifort` [allows 31](http://software.intel.com/sites/products/documentation/hpc/composerxe/en-us/2011Update/fortran/win/lref_for/source_files/rfaf2008k.htm). `gfortran` still does not support more than 7 dimensions as of [May 2013 ](http://gcc.gnu.org/ml/fortran/2013-05/msg00037.html). So - if you **really** need all 22 dimensions, use `ifort` - but be aware that this feature is an extension of the standard, other compilers might not compile your code. \r\n\r\nEDIT: To sum up the discussion in the comments:\r\n\r\nThere are two problems - one related to the maximum dimensions and one related to the array size. \r\n\r\nFor compilers that do not allow for 22 dimensions, one could use one large array of rank 1 that holds all elements and calculate the index inside the element oneself using strides.",
        "score": 23.515625,
        "rank": 16,
        "document_id": "bad66e81-6bbb-4f38-b78f-82bb482d67dc",
        "passage_id": 369757
    },
    {
        "content": "Also you need to consider, that the Intel OpenMP runtime spawns an extra management thread that can confuse the scheduler. There are more choices for pinning, for instance `KMP_AFFINITY=compact` - but for some reason that is totally messed up on my system. You can add `,verbose` to the variable to see how the runtime is pinning your threads.\r\n\r\n[likwid-pin][1] is a useful alternative providing more convenient control.\r\n\r\nIn general single precision should be at least as fast as double precision. Double precision can be slower because:\r\n\r\n - You need more memory/cache bandwidth for double precision.\r\n - You can build ALUs that have higher througput for single precision, but that usually doesn&#39;t apply to CPUs but rather GPUs.\r\n\r\nI would think that once you get rid of the performance anomaly, this will be reflected in your numbers.\r\n\r\nWhen you scale up the number of threads for MKL/*gemm, consider\r\n\r\n - Memory /shared cache bandwidth may become a bottleneck, limiting the scalability\r\n - Turbo mode will effectively decrease the core frequency when increasing utilization.",
        "score": 23.453125,
        "rank": 17,
        "document_id": "4f0504f0-2eb5-4334-bf3f-4e6d7074820e",
        "passage_id": 80272
    },
    {
        "content": "For measurements over long intervals, out-of-order processing across the measurement boundaries is negligible, so CPUID is not needed.)*\r\n 9. Advanced: Using LFENCE in benchmarks is only relevant if you are measuring at very fine granularity -- less than a few hundred cycles.  More notes on this topic at http://sites.utexas.edu/jdm4372/2018/07/23/comments-on-timing-short-code-sections-on-intel-processors/\r\n\r\nIf I assume that your processor was running at its maximum Turbo frequency of 4.6 GHz during the test, then the reported cycle counts correspond to 9.67 milliseconds and 5.23 milliseconds, respectively.  Plugging these into a &quot;sanity check&quot; shows:\r\n\r\n - Assuming that the first case performs one read, one allocate, and one writeback (each 128MiB), the corresponding DRAM traffic rates are 27.8GB/s + 13.9 GB/s = 41.6 GB/s == 108% of peak.",
        "score": 23.4375,
        "rank": 18,
        "document_id": "671e38e2-76e6-4e3f-8835-da01c90e30c1",
        "passage_id": 218776
    },
    {
        "content": "1MBps is very slow for any GPU that can run OpenCL.  However, if we look at the numbers a bit closer, we might get a bit different results.\r\n\r\nFirst off, your sample is not pushing 7MB, it&#39;s pushing two buffers 3.66MB each.  Together they are 7.32MB, which is not a big difference, but nevertheless is a difference.\r\n\r\nHowever there is a bigger uncertainty in this code.  You&#39;re using GetSystemTime().  It reports time with the millisecond precision, but the granularity is not guaranteed.  In fact, if there is no processes that requested higher granularity, you will get up to 55ms accuracy.  This is most likely what&#39;s happening with your code.",
        "score": 23.40625,
        "rank": 19,
        "document_id": "644f0369-4578-4282-9afc-b6b44826bc33",
        "passage_id": 412870
    },
    {
        "content": "`_ps` versions of course also exist.  14 bits of mantissa precision may be enough to use without a Newton iteration in more cases.\n\nA Newton iteration still won&#39;t give you a *correctly rounded* single-precision float the way regular sqrt will.\n\n\n  [1]: http://software.intel.com/en-us/articles/interactive-ray-tracing\n  [2]: http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html\n  [3]: http://www.machinedlearnings.com/2011/06/fast-approximate-logarithm-exponential.html\n  [4]: https://en.wikipedia.org/wiki/AVX-512#CPUs_with_AVX-512",
        "score": 23.359375,
        "rank": 20,
        "document_id": "091ff5aa-de7b-4839-8624-c6bfc7f0a184",
        "passage_id": 78634
    },
    {
        "content": "According to the [PAPI event mapping][4], `PAPI_L1_DCM` and `PAPI_L2_TCM` are mapped to `L1D.REPLACEMENT` and `LONGEST_LAT_CACHE.REFERENCE` performance monitoring events on Intel processors. These are defined in the Intel manual as follows:\r\n\r\n&gt; **L1D.REPLACEMENT**: Counts L1D data line replacements including opportunistic\r\n&gt; replacements, and replacements that require stall-for-replace or\r\n&gt; block-for-replace.\r\n&gt; \r\n&gt; **LONGEST_LAT_CACHE.REFERENCE**: This event counts core-originated cacheable demand requests that refer\r\n&gt; to the last level cache (LLC). Demand requests include loads, RFOs,\r\n&gt; and hardware prefetches from L1D, and instruction fetches from IFU.\r\n\r\nWithout getting into the details of when these events exactly occur, there are three important points here that are relevant to your question:\r\n\r\n - Both events are counted at the cache-line granularity, not x86 instruction or load uop granularities.\r\n - These events may occur due to the L1D hardware prefetchers. This can impact `miss2`.",
        "score": 23.34375,
        "rank": 21,
        "document_id": "742f248b-931b-44fa-aac3-c42551e6af6c",
        "passage_id": 65195
    },
    {
        "content": "By default, `CL_DEVICE_MAX_MEM_ALLOC_SIZE` reports 1/4 of `CL_DEVICE_GLOBAL_MEM_SIZE`, meaning it would only be allowed to allocate four 3GB buffers on a 12GB GPU.\r\n\r\nHowever, Nvidia GPUs allow to allocate their full memory capacity in a single buffer, even though they also report to have the 1/4 limit.\r\n\r\nSome AMD GPUs have the limit set higher, for example the Radeon VII lets you use 14/16GB for a single buffer.\r\n\r\nThe only devices I have ever seen that really inforce the 1/4 limit are Intel HD 4600 and 5500, so older Intel integrated GPUs. If you go above 1/4 in buffer size there, the `cl::Buffer` constructor throws error `-61`.\r\n\r\nIn case you are stuck with the 1/4 memory limit on your device, split your large 12GB buffer in 4 smaller 3GB buffers (for example one vector for x, y, z, w components of the vector each). If you use Windows, note that you might only be able to use ~11.5GB in total as some VRAM is reserved for the operating system.",
        "score": 23.34375,
        "rank": 22,
        "document_id": "0d4a6f5b-1069-4b64-b662-12f681c7da9c",
        "passage_id": 119353
    },
    {
        "content": "The answer to the question is the reply sent by &lt;B&gt;doynax&lt;/B&gt;:\r\n\r\n&lt;code&gt;&quot;I believe there is a shim in recent versions of Windows limiting the available DPMI memory to 32 MB, for yet more ancient software incapable of unprepared such wast quantities of RAM. You may try creating a DpmiLimit key under HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\WOW in the registry to adjust this with the limit in bytes as a DWORD&quot;&lt;/code&gt;",
        "score": 23.3125,
        "rank": 23,
        "document_id": "899e26cb-f1e7-40cd-9b50-a574205ed152",
        "passage_id": 266527
    },
    {
        "content": "4. As you noted, beyond size 32, the times hover around 10ms, which is showing your timing granularity.  You need to either increase the time required (so that a 10ms granularity is sufficient) or switch to a different timing mechanism, which is what the comments are debating.  I&#39;m a fan of using the instruction rdtsc (read timestamp counter (i.e., cycle count)), but this can be even more problematic than the suggestions above.  Switching your code to rdtsc basically required switching clock, clock_t, and CLOCKS_PER_SEC.  However, you could still face clock drift if your thread migrates, but this is a fun test so I wouldn&#39;t concern myself with this issue.\r\n\r\nMore caveats:  the trouble with consistent strides (like powers of 2) is that the processor likes to hide the cache miss penalty by prefetching.  You can disable the prefetcher on many machines in the BIOS (see [&quot;Changing the Prefetcher for Intel Processors&quot;][1]).\r\n\r\nPage faults may also be impacting your results.  You are allocating 500M ints or about 2GB of storage.",
        "score": 23.296875,
        "rank": 24,
        "document_id": "a41d0e6e-046a-4b85-b2f8-6f22da9f8e09",
        "passage_id": 417334
    },
    {
        "content": "If you want a segment to have a specific size in bytes (between 0x00000 and 0xFFFFF) you can use byte granularity. It is possible to have descriptors defined with different granularity.\r\n\r\n  [1]: https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.html\r\n  [2]: https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-2a-manual.pdf",
        "score": 23.28125,
        "rank": 25,
        "document_id": "d5ce05ea-8dfa-4d84-9a56-68f7d47123b0",
        "passage_id": 240132
    },
    {
        "content": "This is because containers typically employ mechanisms unsupported in SYCL device code because they require runtime support, for example throwing exceptions. Because on, say, a GPU there&#39;s no C++ runtime, such mechanisms cannot work in SYCL.\r\n\r\nIt is also important to understand that this is not really a SYCL-specific limitation, but a common restriction among heterogeneous programming models. Other heterogeneous programming models such as CUDA impose similar restrictions for similar reasons.\r\n\r\nAnother difficulty with containers in kernels is that STL data structures are usually not really designed for the massively parallel SIMT execution model on a SYCL device, making them prone to race conditions.\r\n\r\nThe final probem is the one you have already identified: You are copying pointers to host memory. Since you are on oneAPI DPC++, the easiest solution to work with pointer-based data structures is to use the Intel SYCL extension of [unified shared memory](https://github.com/intel/llvm/blob/sycl/sycl/doc/extensions/USM/USM.adoc) (USM) which can be used to generate pointers that are valid both on host and device. There is also a USM allocator that could be passed to containers if they were supported in device code.",
        "score": 23.265625,
        "rank": 26,
        "document_id": "73e0dab8-ff3f-431c-ab69-98152089fd80",
        "passage_id": 211492
    },
    {
        "content": "From the [Intel&#174; 64 and IA-32 Architectures Developer&#39;s Manual: Vol. 2A][2] the _LSL_ instruction has a description of the mechanism used and describes the behavior you are seeing in BOCHS:\r\n\r\n&gt; The segment limit is a 20-bit value contained in bytes 0 and 1 and in the first 4 bits of byte 6 of the segment\r\ndescriptor. If the descriptor has a byte granular segment limit (the granularity flag is set to 0), the destination\r\noperand is loaded with a byte granular value (byte limit). **If the descriptor has a page granular segment limit** (the\r\ngranularity flag is set to 1), the LSL instruction will translate the page granular limit (page limit) into a byte limit\r\nbefore loading it into the destination operand. **The translation is performed by shifting the 20-bit \u201craw\u201d limit left 12\r\nbits and filling the low-order 12 bits with 1s.",
        "score": 23.234375,
        "rank": 27,
        "document_id": "d5ce05ea-8dfa-4d84-9a56-68f7d47123b0",
        "passage_id": 240130
    },
    {
        "content": "Delphi emits code with 64 bit pointers. Such pointers are capable of addressing 2&lt;sup&gt;64&lt;/sup&gt; bytes, that is more than 1TB. Any limits are imposed by the operating system&#39;s virtual memory design and/or licensing and the Delphi compiler knows nothing of such limits. At runtime the system enforces memory limits and your code will be subject to those limits. But Delphi itself has no addressing limitations.\r\n\r\nLikewise, FastMM has no concept of the 1TB limit. FastMM will report out of memory errors when the system reports that memory has been exhausted.\r\n\r\n&gt; Can Delphi / FastMM can address the same as the\r\nspecifications for the OS?\r\n\r\nYes.\r\n\r\n----------\r\n\r\nAn aside. Older 32 bit Delphi versions that used the Borland memory manager were not compatible with addresses in the 2GB-4GB range. This problem manifested if such Delphi code executed in a process marked as large address aware.\r\n\r\nI&#39;m mentioning this to illustrate how it could be that your development tool places limits on you over and above those of the operating system. Now, there are no such problems with 64 bit Delphi but the question that you ask is quite valid.",
        "score": 23.234375,
        "rank": 28,
        "document_id": "0a8f0673-75c0-42d0-9b50-dacae91b4344",
        "passage_id": 404668
    },
    {
        "content": "And there is one more direct hint in &quot;Intel 64 and IA-32 Architectures Software developer manual, vol 3a&quot;, section &quot;10.11.9 Large page considerations&quot;:\r\n\r\n&gt; The MTRRs provide memory typing for a limited number of regions that have a 4 KByte granularity (the same granularity as 4-KByte pages). *The memory type for a given page is cached in the processor\u2019s TLBs.*\r\n\r\nYou asked:\r\n\r\n&gt;  On each memory access does the hardware check whether the physical address falls in a given range \r\n\r\nNo. Every memory access is not compared with all MTRRs. All MTRRs ranges are precombined with PTEs bits of memory when PTE is loaded into TLB. Then the only place to check memory type will be TLB line. And the TLB *IS* checked for every memory access.\r\n\r\n\r\n&gt; whether it should look up the cache or lookup the writecombining buffer or send it to memory controller directly\r\n\r\nNo, there is something that we don&#39;t understand clearly.",
        "score": 23.203125,
        "rank": 29,
        "document_id": "728d1bb5-9223-4297-997d-8db02ffbbfac",
        "passage_id": 434205
    },
    {
        "content": "The enumerator `GL_MAX_3D_TEXTURE_SIZE` defines how big 3D textures can be, and 4.6 implementations are required to support at least 2048 per dimension.\r\n\r\nOf course, this says nothing about *memory* limitations.\r\n\r\nOpenGL implementations are not required to fulfill the execution of any function immediately (unless the function specifically says it does). They can defer things like memory allocation until whenever they like. As such, the error `GL_OUT_OF_MEMORY` can be thrown *at any time*, regardless of when the operation that actually runs out of memory was executed.\r\n\r\n&gt; I just want to be told when I am trying to do something the system can&#39;t handle.\r\n\r\nThis is why Vulkan exists; OpenGL is simply not equipped to answer those questions immediately.",
        "score": 23.171875,
        "rank": 30,
        "document_id": "f34c9ee7-1d0e-48e1-b627-810f678d6434",
        "passage_id": 267368
    },
    {
        "content": "However, before you fix this problem you should know...\r\n\r\n3. The strategy &quot;`Play()`, wait, sleep, `Play()`&quot; is never going to achieve precise timing of one stimulus relative to the other anyway.  First, whenever you issue a command to play a sound in any software, there will unavoidably be a non-zero (and randomly varying!) latency between the command and the physical onset of the sound. Second, `sleep()` is unlikely to be as precise as you think it is. This applies both to the `sleep()` you\u2019ve been using to create a gap, and also to the `sleep()` that would be used internally by `Play(wait=True)`. Sleep implementations suspend operation for &quot;at least&quot; the specified amount of time but they don&#39;t guarantee an upper bound on that. This is very hardware- and OS-dependent; on some Windows systems you may even find that the granularity *never* gets any better than 10ms.",
        "score": 23.15625,
        "rank": 31,
        "document_id": "be4418c4-b23b-4dc2-acda-4cdd8bed39c3",
        "passage_id": 43775
    },
    {
        "content": "When your application in a pod reaches the limits of memory you set by resources.limits.memory or namespace limit, Kubernetes restarts the pod.\r\n\r\nThe Kubernetes part of limiting resources is described in the following articles:\r\n\r\n- [Kubernetes best practices: Resource requests and limits][5]\r\n- [Resource Quotas][6]\r\n- [Admission control plugin: ResourceQuota][7]\r\n- [Assign Memory Resources to Containers and Pods][8]\r\n\r\nMemory consumed by Java application is not limited to the size of the Heap that you can set by specifying the options:\r\n\r\n    -Xmssize Specifies the initial heap size.\r\n    -Xmxsize Specifies the maximum heap size.\r\n\r\nJava application needs some additional memory for metaspace, class space, stack size, and JVM itself needs even more memory to do its tasks like garbage collection, JIT optimization, Off-heap allocations, JNI code.\r\nIt is hard to predict total memory usage of JVM with reasonable precision, so the best way is to measure it on the real deployment with usual load.\r\n\r\nI would recommend you to set the Kubernetes pod limit to double `Xmx` size, check if you are not getting OOM anymore, and then gradually decrease it to the point when you start getting OOM.",
        "score": 23.125,
        "rank": 32,
        "document_id": "3dd10a1c-a365-4d84-b473-f541f5257124",
        "passage_id": 114246
    },
    {
        "content": "Sleep implementations suspend operation for &quot;at least&quot; the specified amount of time but they don&#39;t guarantee an upper bound on that. This is very hardware- and OS-dependent; on some Windows systems you may even find that the granularity *never* gets any better than 10ms.\r\n\r\nIf you really want to use the `Synth` approach I suppose you could program the gap procedurally into the function definitions of `tone440Hz()` and `tone880Hz()`, accessing `cue_duration`, `duration` and `spacing` as global variables (in fact, while you&#39;re at it, why not make frequency a global variable too, and only write one function). But I don&#39;t see any great advantage in this, either in performance or in code maintainability.",
        "score": 23.125,
        "rank": 33,
        "document_id": "be4418c4-b23b-4dc2-acda-4cdd8bed39c3",
        "passage_id": 43776
    },
    {
        "content": "Oh, and note that IEEE-754 does not place constraints on the precision, let alone require implementation, of &quot;additional operations&quot; (e.g. sin, cos, tanh, atan, ln, etc); it purely recommends them. \r\n\r\n* See, a very common case where this goes wrong and sin gets quantized to 4 different values on intel integrated graphics, but otherwise has reasonable precision on alternative hardware: https://stackoverflow.com/questions/21464664/sinx-only-returns-4-different-values-for-moderately-large-input-on-glsl-fragme\r\n\r\nAlso, note that Unity does not guarantee that a `float` in shader is actually a 32-bit float; on certain hardware (e.g. mobile), it can even be backed by a 16-bit `half` or an 11-bit `fixed`.\r\n\r\n&gt; High precision: float\r\n&gt; Highest precision floating point value; generally 32 bits (just like float from regular programming languages).\r\n&gt;\r\n&gt; ...\r\n&gt; One complication of float/half/fixed data type usage is that PC GPUs are always high precision.",
        "score": 23.125,
        "rank": 34,
        "document_id": "3dd23c55-9b35-45cd-b5ac-f6ffd030e4fb",
        "passage_id": 40858
    },
    {
        "content": "The purpose of CUDA&#39;s device function intrinsics is to expose particular hardware capabilities that are not otherwise accessible, e.g. fast approximations to some algebraic and transcendental functions, or clamping to the interval [0,1] in the case of `__saturatef()`, which maps to GPU&#39;s machine instruction `F2F{.FTZ}.F32.F32.SAT` for compute capabilities prior to 5.0 (`sm_50)` and `FADD.SAT` for architectures greater than, or equal to, compute capability 5.0.\r\n\r\nA look at the [PTX documentation](http://docs.nvidia.com/cuda/parallel-thread-execution/index.html) shows that floating-point saturation is supported for half-precision (`.f16`) and single-precision (`.f32`) operations, but not for double-precision (`.f64`) operations. So the non-orthogonality of the provided intrinsics is caused by non-orthogonality in the GPU hardware. Saturation is provided for lower precisions because of relevant use cases, including graphics, which typically do not use double precision.",
        "score": 23.109375,
        "rank": 35,
        "document_id": "2a1e5b28-8684-4e06-841c-214d421c0fde",
        "passage_id": 5958
    },
    {
        "content": "Indeed, the number of pending cache-line request per core is limited (to a dozen of them on Skylake), so the kernel is bound by the latency of the requests since there is not enough concurrency to fully overlap their latency.\r\n\r\nFor the DAXPY/copy, hardware prefetchers can better reduce the latency but the [amount of concurrency is still too small compared to the latency on Xeon processor so to fully saturate the RAM with 1 thread](https://stackoverflow.com/questions/39260020/why-is-skylake-so-much-better-than-broadwell-e-for-single-threaded-memory-throug). This is a quite reasonable architectural limitation since such processors are designed to execute applications scaling well on many cores.\r\n\r\nWith many threads, the per-core limitation vanishes and it is replaced by a stronger one: the practical RAM bandwidth.\r\n\r\n&gt; Could it be that the Intel compiler performed loop-blocking for the naive transpose function as optimization?\r\n\r\nThis is theoretically possible since the Intel compiler (ICC) has such optimizer, but it is very unlikely for ICC to do that on a 3D transposition code (since it is a pretty complex relatively specific use-case). The best is to **analyse the assembly code** so to be sure.",
        "score": 23.03125,
        "rank": 36,
        "document_id": "167cd78f-6af0-4ce2-8468-72c49d406772",
        "passage_id": 37587
    },
    {
        "content": "- the EPC size is limited on current systems to roughly 90MB\r\n- Windows does not currently support swapping out these pages\r\n- an enclave must request all pages it wishes to use before executing (EINIT) on SGXv1 hardware\r\n- the size of all enclaves must not exceed the EPC size\r\n- Intel reserves some EPC space for their management enclaves (quoting, provisioning, loading enclaves)\r\n\r\nSo your enclave will have to use well below 90MB of heap size on current hardware. I have experimented with the SDK emulation, and found that it allows a heap max size of roughly 1GiB [2]. Future OS versions will hopefully support EPC page swapping, allowing larger static enclave sizes. Future SGX hardware will allow dynamic page allocation, allowing dynamic enclave sizes.\r\n\r\n[1] https://software.intel.com/en-us/forums/intel-isa-extensions/topic/607004#comment-1857071\r\n\r\n[2] 1GiB - 64KiB - TCSnum * 128KiB, where TCSnum is the number of threads. Exceeding this HeapMaxSize results in a *simulation error*",
        "score": 23.0,
        "rank": 37,
        "document_id": "cac521c8-b6cf-421f-a288-37f98d20e70d",
        "passage_id": 266439
    },
    {
        "content": "I&#39;m not sure just which limit you&#39;re talking about, and that may well be system dependent. In Linux, the limit on how much can be mapped depends on RLIMIT_AS and kernel configuration. Depending on memory layout, it is common that you can mmap more than you can malloc (heap allocation, where most Python objects reside). \r\n\r\nThe per-mmap limits could be system dependent or simply depend on what contiguous ranges are still available in your virtual memory. A look at `/proc/$$/maps` in Linux, or a debugging tool such as MHS on Windows, will inform you how that looks. The primary limitation is that the offset passed to mmap must be a multiple of `mmap.ALLOCATIONGRANULARITY`. Since each mmap must find a gap to fit into within the virtual memory map, multiple mmaps often can exceed the total size of a single possible mmap. Some additional constraints may be in place, such as auto-allocated addresses only being in a certain range, and ranges being restricted to kernel use.",
        "score": 23.0,
        "rank": 38,
        "document_id": "19531ae7-acde-4573-b66e-bb61f6ca6e8a",
        "passage_id": 405487
    },
    {
        "content": "**2) What are the specific reasons for this empty address space?**\r\n\r\nThe overriding reason for aligning memory data is for software efficiency and robustness. As discussed, the processor will access memory at the granularity of its word size.   \r\n\r\n**3) How is the size of the space calculated?**\r\n\r\nThe assembler will pad out the section, so that the data immediately following on from it, is automatically aligned to an instance of the specified memory access boundary. In the original question, `section .data` would have ended at address `0x\u2026 60012a`, in the absence of the necessary padding, with `section .bss` starting at address 60012b.  Here, the data would not have been properly aligned with the memory access boundary defined by the CPU&#39;s access granularity. Consequently, NASM, in its wisdom, adds a padding of one `nul` character, in order to round the memory address up to the next address that *is* divisible by 4, and hence, properly align the data.",
        "score": 22.984375,
        "rank": 39,
        "document_id": "23d92a63-dee5-4de7-a3c9-cb522cf03bc9",
        "passage_id": 203279
    },
    {
        "content": "All modern languages do bounds checking in software, on top of segment bounds checking and memory map lookups. [One benchmark](http://blogs.msdn.com/b/ricom/archive/2006/07/12/663642.aspx) suggests the overhead is about 0.5%. This is a small price to pay for stability and security. \r\n\r\nA 486 could [load a memory location in a single cycle](http://zsmith.co/intel_m.html#mov), and CPUs have only gotten better at doing on-chip processing, so it&#39;s unlikely that segmentation bounds checking has any overhead at all. \r\n\r\nStill, though, you can simply run in 64bit mode: &quot;The processor does not perform segment limit checks at runtime in 64-bit mode&quot; [(Intel&#39;s developer manual, 3.2.4)](http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-manual-325462.pdf).",
        "score": 22.96875,
        "rank": 40,
        "document_id": "1b9f4646-3ddc-44f7-9d13-7cfbe065cfe0",
        "passage_id": 387194
    },
    {
        "content": "This inference is supported by the weak suggestion in the documentation (link below) that on OSX, modified pages would be written to the backing store, regardless of whether or not they are synchronized with the disk. I intend to do a bit more reading on the Mach kernel to more fully understand these limitations, however.\r\n\r\nSo `mmap` with `PROT_READ` appears to be our path to using all of our process&#39;s available VM space without hitting resident set size limits, right? I mean, with this method you could in theory create read-only mappings of some large file many times over, with each mapping occupying a separate portion of your process&#39;s available logical memory space. On a 32bit OS, even on 32bit iOS, you can likely achieve your poorly-thought-out plans of world domination by virtual address space saturation quite easily. `ENOMEM` for everyone!",
        "score": 22.9375,
        "rank": 41,
        "document_id": "c0320a20-c707-42e1-826c-9b4b0ca8e78b",
        "passage_id": 308714
    },
    {
        "content": "The most typical reason for this is that not enough space is available on the stack to hold the private copy of `numstrain`. Compute and compare the following two values:\r\n\r\n* the size of the array in bytes\r\n* the stack size limit\r\n\r\nThere are two kinds of stack size limits. The stack size of the main thread is controlled by things like process limits on Unix systems (use `ulimit -s` to check and modify this limit) or is fixed at link time on Windows (recompilation or binary edit of the executable is necessary in order to change the limit). The stack size of the additional OpenMP threads is controlled by environment variables like the standard `OMP_STACKSIZE`, or the implementation-specific `GOMP_STACKSIZE` (GNU/GCC OpenMP) and `KMP_STACKSIZE` (Intel OpenMP).\r\n\r\nNote that most Fortran OpenMP implementations always put private arrays on the stack, no matter if you enable compiler options that allocate large arrays on the heap (tested with GNU&#39;s `gfortran` and Intel&#39;s `ifort`).",
        "score": 22.859375,
        "rank": 42,
        "document_id": "4bf2a156-3cf1-41c4-9371-9247c41c46e8",
        "passage_id": 352012
    },
    {
        "content": "So you cannot expect this interface to be a cross-platform solution, and the 15.625ms limit still applies.\r\n\r\n\r\nKnowing that, how can we deal with it? Well, one can use the `timeBeginPeriod()` thing to increase precision of some timers but, again, we are not advise to do so: **it seems better to use `QueryPerformanceCounter()` (QPC)**, which is the primary API for native code  looking forward to [acquire high-resolution time stamps or measure time intervals][5] according to Microsoft. Note that [GPC does count elapsed time (and not CPU cycles)][6]. Here is a usage example:\r\n\r\n    LARGE_INTEGER StartingTime, EndingTime, ElapsedMicroseconds;\r\n    LARGE_INTEGER Frequency;\r\n    \r\n    QueryPerformanceFrequency(&amp;Frequency); \r\n    QueryPerformanceCounter(&amp;StartingTime);\r\n    \r\n    // Activity to be timed\r\n    \r\n    QueryPerformanceCounter(&amp;EndingTime);\r\n    ElapsedMicroseconds.QuadPart = EndingTime.QuadPart - StartingTime.QuadPart;\r\n    \r\n    \r\n    //\r\n    // We now have the elapsed number of ticks, along with the\r\n    // number of ticks-per-second. We use these values\r\n    // to convert to the number of elapsed microseconds.",
        "score": 22.84375,
        "rank": 43,
        "document_id": "1b6983c7-0660-4de5-90cc-879809a6c571",
        "passage_id": 84895
    },
    {
        "content": "&gt; why degrade the coordinates at the last step?\r\n\r\n... because GPUs can do a few billion texture filtering operations per second, and you really don&#39;t want to waste power and silicon area doing calculations at fp32 precision if all practical use cases need 8-bit fixed point. \r\n\r\nNote this is 8-bits of _sub-texel_ accuracy (i.e. granularity for GL_LINEAR filtering between two adjacent texels). Selecting texels is done at whatever higher precision is needed (most modern GPUs can uniquely address 16K textures, with 8-bits of subtexel accuracy).",
        "score": 22.84375,
        "rank": 44,
        "document_id": "9d5a9b83-5c15-4e14-8f4f-2de12051acd3",
        "passage_id": 36333
    },
    {
        "content": "The guarantee that a T[N][M] will contain NxM consecutive objects of type T impedes some otherwise-useful optimizations; the primary usefulness of that guarantee in pre-standard versions of C was that it allowed code to treat storage as a single-dimensional array in some contexts, but a multi-dimensional array in others.  Unfortunately, the Standards fails to recognize any distinction between the pointers formed by the decay of an inner array versus a pointer formed by casting an outer array to the inner-element type either directly or through `void*`, even though they impose limitations on the former which would impede the usefulness of the latter.\r\n\r\nOn any typical platform, in the absence of whole-program optimization, the ABI would treat a pointer to an element of a multi-dimensional array as equivalent to a pointer to an element of a single-dimensional array with the same total number of elements, making it safe to treat the latter as the former.",
        "score": 22.84375,
        "rank": 45,
        "document_id": "b5ab178b-ca90-4632-873b-9558fbaabc30",
        "passage_id": 234737
    },
    {
        "content": "------------\r\n\r\n&lt;sup&gt;1 - Any reasonable JVM implementation will know that local variables go out of scope when a method exits.  Whether the JVM tracks scopes at a finer granularity is implementation specific, and (to be honest) I *do not know* how JVMs handle this in practice. &lt;/sup&gt;\r\n\r\n&lt;sup&gt;Note that just about anything is *technically* conformant to the JLS requirements .... provided that the GC does not delete reachable (i.e. non-garbage) objects. That includes a JVM that uses the Epsilon no-op GC, which never collects garbage and terminates the JVM when it runs out of space.&lt;/sup&gt;\r\n\r\n&lt;sup&gt;2 - The only legitimate reasons are: 1) testing the behavior of GC related functionality; e.g. finalizers, reference queue processors, etc, or 2) avoiding harmful GC pauses in a real-time application; e.g. running the GC when changing &quot;levels&quot; in a real-time game.&lt;/sup&gt;",
        "score": 22.8125,
        "rank": 46,
        "document_id": "c88c0769-1aeb-49f9-b924-6cb4354505f1",
        "passage_id": 113788
    },
    {
        "content": "You just need to adapt the script to specify the Host entity instead of the Guest (or allow both) when you perform the query.\r\n\r\nFinally, the &quot;real-time&quot; granularity for the retrieved metrics can be specified by a time interval when performing the query call. You should specify an interval based on your needs, but note that for the latest vSphere versions, the minimum (default) interval between samples is 20 seconds, so you should not specify a time interval lesser than this value.\r\n\r\n\r\n  [1]: https://github.com/vmware/pyvmomi-community-samples\r\n  [2]: https://github.com/lgeeklee/python-vmstats",
        "score": 22.8125,
        "rank": 47,
        "document_id": "d10978de-f3c0-4597-8ddd-8217939de182",
        "passage_id": 145401
    },
    {
        "content": "That is the correct (and only) way to do it. The detection in v2.0 and earlier is limited to the 500 ms accuracy.\r\n\r\n\r\nThe detection algorithm is improved in coming release 2.1 which will contain:\r\n\r\n - instant notification on Leads-ON when not measuring HR/RR/ECG\r\n - 500ms accuracy of Leads-OFF when not measuring HR/RR/ECG (limitation of current hardware)\r\n - notification on Leads-ON and Leads-OFF when measuring HR/RR/ECG (fast on ECG measurement, slower on HR/RR)\r\n\r\n\r\n*Full disclosure: I work for the Movesense team*",
        "score": 22.8125,
        "rank": 48,
        "document_id": "033594dd-fa31-4631-a7a6-c79ed9b0a7a3",
        "passage_id": 46120
    },
    {
        "content": "But `movzx` is not a bad idea.\r\n\r\nWhen all else is equal, smaller code-size is almost always better (for instruction-cache footprint, and sometimes for packing into the uop cache).  If it cost any extra uops or introduced any false dependencies, it wouldn&#39;t be worth it when optimizing for speed, though.\r\n\r\n\r\n  [1]: https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html\r\n  [2]: https://software.intel.com/sites/default/files/managed/9e/bc/64-ia-32-architectures-optimization-manual.pdf#page=435\r\n  [3]: https://stackoverflow.com/questions/47300844/mach-o-64-bit-format-does-not-support-32-bit-absolute-addresses-nasm-accessing\r\n  [4]: https://www.felixcloutier.com/x86/test",
        "score": 22.765625,
        "rank": 49,
        "document_id": "3ab45a56-06ac-474c-9d58-737c7ac777ad",
        "passage_id": 127254
    },
    {
        "content": "&gt; Does this feature still exist in any capacity in the modern Nsight toolset?\r\n\r\nThis feature is supported by the Visual Profiler and `nvprof`. Both these tools are supported up to NVIDIA Volta architecture.\r\n\r\n&gt; what is the recommended method of evaluating power consumption of GPU code?\r\n\r\nYou can use `nvidia-smi dmon` option `-s`/`--select`, if granularity in the range of seconds is sufficient. You could check `nvidia-smi dmon -h`.",
        "score": 22.765625,
        "rank": 50,
        "document_id": "267f7bc5-8f1b-4359-a83e-00d8bb397793",
        "passage_id": 1750
    }
]