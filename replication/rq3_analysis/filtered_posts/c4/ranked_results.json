[
    {
        "content": "Therefore this frequency cannot be considered as a constant. This way measurements are only posible during constant phases.\r\n\r\nIn fact none of the frequency sources delivers a constant frequency. All of these frequencies are generated by some hardware which has `offset and drift`. So the OS will return a value for QueryPerformanceFrequency or CPU frequency and makes you believe it is a constant. However, the number you&#39;ll get are only `close estimates`.\r\n\r\nReal accurate timing can only be performed when these frequencies are calibrated against the systems RTC. See [this][1] publication for more detailed information about accurate timing on windows.\r\n\r\n**Edit:** Windows chooses the `Time Stamp Counter` of the cpu. In such cases the result of QPF() equals the processor speed divided by a fixed number (1024 in your case). Windows chooses to built the timekeeping around the TSC with preference when a constant/invariant TSC is available. 3.33 GHz/1024=3.25 MHz.\r\n\r\n  [1]: http://www.windowstimestamp.com/description",
        "score": 24.046875,
        "rank": 1,
        "document_id": "4301882c-1d65-4e17-9e5a-8ec57048a83c",
        "passage_id": 102049
    },
    {
        "content": "0x0080  hold frequency (rw)\r\n    \r\n    STA_PPSSIGNAL   0x0100  PPS signal present (ro)\r\n    STA_PPSJITTER   0x0200  PPS signal jitter exceeded (ro)\r\n    STA_PPSWANDER   0x0400  PPS signal wander exceeded (ro)\r\n    STA_PPSERROR    0x0800  PPS signal calibration error (ro)\r\n    \r\n    STA_CLOCKERR    0x1000  clock hardware fault (ro)\r\n    STA_NANO        0x2000  resolution (0 = us, 1 = ns) (ro)\r\n\r\nSource: ftp://ftp.ripe.net/test-traffic/ROOT/libDelay/Delay.h\r\n\r\nFrom your listed example: `status 0x2107 (PLL,PPSFREQ,PPSTIME,PPSSIGNAL,NANO)`.",
        "score": 23.3125,
        "rank": 2,
        "document_id": "e97e1e5d-ea6f-43d7-8b0e-03466c270a7e",
        "passage_id": 78333
    },
    {
        "content": "We found the issue.\r\nThe Problem was that `VDDA` pin had not enough energy supplied to it.\r\nThis means, it was a hardware issue.\r\n\r\n\r\nTo make clear what the actual problem was and why we didnt detect it:\r\n\r\nWe checked all `VDD(A)` pins and found that every pin had the right voltage supplied. However, in the gerber files we discovered that the `VDDA` pin via was not connected properly to the 3.3V plane. There was a huge resistance (in this case 600k\u03a9) between the 3.3V rail and the `VDDA` pin. We measured the right voltage at the `VDDA` pin because there was a capacitor and it had enough time to charge up. Thus we measured the correct voltage. However, once the uC started to draw actual power, the cap didnt provide enough energy/current and the uC couldnt start up. That also explained the error message from JLINK &quot;Couldnt start up DAP&quot;.\r\n\r\nErgo: `DEV_TARGET_CMD_ERR` is probably a hardware issue.",
        "score": 23.03125,
        "rank": 3,
        "document_id": "d706dc7b-10c0-4975-8b65-1b27312bb7d5",
        "passage_id": 134716
    },
    {
        "content": "You cannot measure to 1 nanosecond, you cannot measure to 10 nanosconds either. This is because each `action of measurement` requires a call of some kind. One of the fastest APIs is \r\nGetSystemTimeAsFileTime(). A call requires 10-15ns. But it&#39;s resolution/granularity is rather poor (in the ms regime). QueryPerformanceCounter() delivers frequencies in the MHz to GHz range, depending on the underlaying hardware. This call is not as fast but at 1MHz you get 1 microsecond resolution. At such a frequency, given by QueryPerformanceFrequency(), consecutive call will may return equal values because the call is faster than the increment rate. \r\nAnother source is the CPU time stamp counter (rdtsc). But there are some drawback with it too: Modern hardware implements adpative CPU frequency. Therefore this frequency cannot be considered as a constant. This way measurements are only posible during constant phases.\r\n\r\nIn fact none of the frequency sources delivers a constant frequency. All of these frequencies are generated by some hardware which has `offset and drift`.",
        "score": 22.78125,
        "rank": 4,
        "document_id": "4301882c-1d65-4e17-9e5a-8ec57048a83c",
        "passage_id": 102048
    },
    {
        "content": "You can use hardware performance counter like [perf][1].\n\nModern hardware provides hardware performance counter, so there are pretty good chances that your machine has this feature. Also you might need to put kernel drivers for it. (on ubuntu sudo apt-get is magic for this).\n\nA little description of perf (imported from above link)\n\n&gt; Perf is a profiler tool for Linux 2.6+ based systems that abstracts\n&gt; away CPU hardware differences in Linux performance measurements and\n&gt; presents a simple commandline interface. Perf is based on the\n&gt; perf_events interface exported by recent versions of the Linux kernel.\n&gt;\n&gt; The perf tool supports a list of measurable events\n\n   where those events are listed in above link.\n\nSay out of many events, you want to get stats of `cycles` event for below \n\n `perf stat -e cycles:u -e cycles:k -e cycles dd if=/dev/zero of=/dev/null count=100000`\nwhere `u` and `k` stands user and kernel respectively. you can also put all the events by separating them using comma.\n\n\n\n\n  [1]: https://perf.wiki.kernel.org/index.php/Tutorial",
        "score": 22.671875,
        "rank": 5,
        "document_id": "24901a32-c325-403c-9a80-3eb290403dc8",
        "passage_id": 446560
    },
    {
        "content": "* modern processors use branch prediction and/or speculative execution, which is a form of instruction cacheing that affect the cycle count in substantial ways and is typically non reproducible.\r\n* the hardware may interfere too: depending on CPU temperature, the clock speed and/or CPU voltage may be adjusted by the motherboard, affecting the run times and possibly the clock counts.\r\n* recent CPUs may even include non predictive adjustments for instructions that read the clock counts or use some other precise timing measurements in order to prevent [side channel attacks][1] that rely on clock measurements to determine protected memory contents.\r\n\r\nPrecise timings of selected program fragments are still possible but not down the the clock count and require advanced skills to try and prevent interference from the above issues and others.  Repeating short tests a large number of times and keeping the best times is a good start, but will always include some range of uncertainty, so with limited accuracy.\r\n\r\nMore important aspects of the algorithm must be studied: time and space complexity, best, average and worst case scenarios, and above all, **correctness** and range limitations.\r\n\r\n\r\n  [1]: https://en.wikipedia.org/wiki/Side-channel_attack",
        "score": 22.453125,
        "rank": 6,
        "document_id": "c36feaa8-3d9b-479f-8d06-256a67edf4d7",
        "passage_id": 32598
    },
    {
        "content": "&gt; Is it possible to calculate that using result from memtest?\r\n\r\nNo.\r\n\r\n&gt; If not, how is that calculated in then?\r\n\r\nThe source code of the tools used to produce the results shown on https://www.7-cpu.com/  is publicly available, which can be found at https://www.7-cpu.com/utils.html. In particular, the MemLat tool is used to measure the access latency to each level of the memory hierarchy.\r\n\r\nThe mainstream method for measuring latency is using pointer chasing, where a linked list of 64-byte elements is created and each element is initialized to basically point to another randomly chosen element (to defeat hardware prefetchers). If the total size of the linked list fits in the L1 cache, then by iterating over the list a sufficiently large number of times, an L1 latency can be measured by dividing the total execution time by the number of elements accessed. This microbenchmark can be simplified by [disabling hardware prefetchers][1] so that there is no need for randomization.",
        "score": 22.390625,
        "rank": 7,
        "document_id": "5738be0f-2c8e-41f6-9bba-2aaae78cdc35",
        "passage_id": 239878
    },
    {
        "content": "A counter reports OVERFLOW if the physical hardware counter reached its maximum value during the capture and the profiler cannot determine a correct value. The majority of hardware counters on NVIDIA GPUs are 32 bits. In Maxwell the SM counter width was increased to ~40 bits.\r\n\r\nA number of the PM experiments can increment a physical counter by 6 bits (0-63) per cycle. At 1 GHz a 32-bit counter has a minimum overflow time of ~68ms. In practice many of the more complex experiments will overflow when kernels exceed 1 second.\r\n\r\nIn order to avoid overflow the developer may have to reduce the execution time of the kernel by reducing the data set or breaking the kernel into multiple launches.\r\n\r\nThe NVIDIA tools teams are working on multiple software and hardware solutions to eliminate overflow issues in longer running kernels. Unfortunately, these solution take time to implement.",
        "score": 22.0625,
        "rank": 8,
        "document_id": "411a4227-0d11-4d06-bc5e-ea6b39a5d81d",
        "passage_id": 23548
    },
    {
        "content": "634275000 seconds user\r\n       0.013270000 seconds sys\r\n```\r\n(I normally run single-threaded tests under `taskset -c 1` to ensure no `cpu-migration` events, but that typically doesn&#39;t happen anyway for short runs on an idle system.)\r\n\r\nMy EPP `/sys/devices/system/cpu/cpufreq/policy*/energy_performance_preference`) settings is `balance-performance` (not full `performance`), so hardware P-state management clocked down to 2.7GHz on this memory-bound workload.  The calculated 2.68GHz only counts user-space cycles because of `--all-user`, but `task-clock` is wall-clock time. (This somewhat reduces per-core memory bandwidth since the uncore slows down, making latency x max_in-flight_lines a limiting factor in single-core memory bandwidth.  This isn&#39;t a problem for this experiment, but it&#39;s something else non-obvious that&#39;s visible in this perf data.  My i7-6700k has dual channel DDR4-2666, running Arch Linux, kernel 6.4.9)\r\n\r\n**Footnote 1: even `--all-user` isn&#39;t perfect**.",
        "score": 22.0625,
        "rank": 9,
        "document_id": "69f0c4d9-e26a-4182-902d-190cc2df1603",
        "passage_id": 124585
    },
    {
        "content": "The big difference for calibration is going to be because the comparison value is fixed at 0 and you can only adjust the reload value, you might have more latency compared to adjusting the comparison value directly (assuming the counter reload happens at the same time the interrupt is generated).\r\n\r\nThe read-only value in `SYST_CALIB` (if indeed it even exists, being implementation-defined and optional), is merely for relating SYSTICK ticks to actual wallclock time - when first initialising the timer, you need to know the tick frequency in order to pick an appropriate reload value for your desired period, so having a register field that says &quot;this many reference clock ticks happen in 10ms (possibly)&quot; offers some possibility of calculating that at runtime in a portable fashion, rather than having to hard-code a value that might need changing for different devices.\r\n\r\nIn this case, however, not only does having an even-more-accurate external clock to synchronise against makes this less important, but crucially, _the firmware has already configured the timer for you_.",
        "score": 22.015625,
        "rank": 10,
        "document_id": "091b65b7-74b0-46cc-a973-a2698a81a6b6",
        "passage_id": 93644
    },
    {
        "content": "It turns out the answer is hardware dependent.  For my hardware for which the compiler defines `__gfx1030__` the correct syntax is\r\n```\r\nasm volatile (&quot;v_add_co_u32 %0, vcc_lo, %1, %2;&quot; : &quot;=v&quot;(r) : &quot;v&quot;(a), &quot;v&quot;(b));\r\n```\r\nFor earlier architechtures such as `__gfx900__` replace `vcc_lo` with `vcc`  \r\nSee the discussion [on the Rocm Hip Github][1] and this [AMD gpu assembly reference][2]. \r\n\r\n\r\n  [1]: https://github.com/ROCm-Developer-Tools/HIP/issues/2564\r\n  [2]: https://llvm.org/docs/AMDGPU/AMDGPUAsmGFX10.html",
        "score": 21.953125,
        "rank": 11,
        "document_id": "a95fbf61-6045-4518-ae64-8e4544e39e48",
        "passage_id": 5235
    },
    {
        "content": "Short answer: No.\r\n\r\nLong answer: Yes, but...\r\n\r\n* GPU parameters are set via your GPU&#39;s driver&#39;s hardware control interface. This interface is unrelated to graphics APIs like OpenGL and Direct3D.\r\n* The hardware control interface is generally undocumented because GPU makers don&#39;t want to have to support third-party developers or those offering hardware modification tools (not least because of the very real risk of hardware damage from incorrect overclocking and the risk of fraudulent warranty returns).\r\n* Overclocking GUIs and GPU control panels are examples of userland programs that use the driver&#39;s hardware interface. The developers of third-party overclocking tools will use computer software reverse-engineering techniques to see what the first-party software (e.g. GPU control panel) does. This is necessary because the hardware control interfaces are not documented (i.e. there&#39;s no C `.h` header files and `.lib` files that tell a compiler what functions do what and where they&#39;re located.\r\n* To use any programming interface from C# requires either a CIL metadata assembly (e.g.",
        "score": 21.9375,
        "rank": 12,
        "document_id": "5a0187db-826e-402d-ad27-c6c5971d4995",
        "passage_id": 252177
    },
    {
        "content": "You didn&#39;t quantify significant, so the following is just speculation...\r\n\r\nYou mention writing to flash.  One of the issue is that writing to flash typically requires the driver to poll the status of the hardware to make sure the operation completes successfully.\r\n\r\nIt is possible that during certain operations, the file system temporarily disables preemption to insure that no corruption occurs - coupled with having to wait for hardware to complete, this might account for the delay.\r\n\r\nIf you have access to the System Viewer tool, that would go a long way towards identifying the cause of the delay.",
        "score": 21.921875,
        "rank": 13,
        "document_id": "621e2070-73cd-4ed5-bcb3-a5171ca84185",
        "passage_id": 444980
    },
    {
        "content": "1. It might be the case your server isn&#39;t capable of responding fast enough. Low CPU and Memory consumption means that the server has enough headroom, however the [application server][1] configuration can be incorrect so the server doesn&#39;t fully utilise it&#39;s hardware resources. Another reason could be inefficient algorithms used in your application code, you can use a [profiler tool][2] to see what&#39;s going on when you&#39;re conducting the load\r\n 2. It might be the case JMeter is not capable of sending requests fast enough. Make sure that the machine where JMeter is running is not overloaded, that you run your JMeter test in non-GUI mode and in general follow [JMeter Best Practices][3]. You can also try running JMeter in [distributed mode][4] in case if one machine is not capable of creating the required load.",
        "score": 21.765625,
        "rank": 14,
        "document_id": "0e64b9a8-9bad-47cc-a6f4-8c75320f97e8",
        "passage_id": 247161
    },
    {
        "content": "This is done for performances reasons, as the hard drive is the bottleneck. Hardware controllers do have batteries to guarantee that their own cache can be flushed even in an event of power loss.\r\n\r\nThe size of a sector is another important factor, but this detail should not be taken into account as the hard drive itself could lie about its native size for interoperability purposes.\r\n\r\nIf you have a mewmory mapped and you insert data in the middle, while the power goes down, the content of the file might partially contain the change you did if it exceeds the size of the internal buffers.\r\n\r\nTxF is a way to mitigate the issue, but has several implications which limits the contexts where you can use it: for eaxample it does not work on different drives or shared networks.\r\n\r\nIn order to be ACID, you need to design your data structures and/or the way you use it in order not to rely about the implementation details. For example, Mercurial (versioning tool) always appends its own data to its own revision log.\r\nThere are many possible patterns, however, the more guarantees you need, the more technology specific you&#39;ll get (and by tied to).",
        "score": 21.625,
        "rank": 15,
        "document_id": "680ce74a-6f35-4d27-8075-729d3d75ca61",
        "passage_id": 248481
    },
    {
        "content": "Instead, expose the hardware clock device on the NIC directly as a HW PTP clock source, to allow time conversion in userspace and optionally synchronize system time with a userspace PTP stack such as linuxptp. For the PTP clock API, see Documentation/driver-api/ptp.rst.\r\n\r\nSee 2.1 from `Documentation/networking/timestamping.txt` for detail.\r\n\r\nIf you want to see HW timestamp then you need to have a specific HW (refer [this comment](https://stackoverflow.com/questions/69888516/how-to-measure-precise-or-approximate-latency-of-receiving-packet#comment123543061_69888516)) and turn its feature with `ioctl()`. However, there is a convenient tool called `linuxptp`, which does this job.",
        "score": 21.59375,
        "rank": 16,
        "document_id": "5dbc0ff3-58fa-46a7-93f8-4e48ae9c1ca4",
        "passage_id": 44212
    },
    {
        "content": "You first have to determine if your problem is on the Apache Server, on the PHP code, on the mySQL database queries or even on mySQL database parameters. \r\n\r\nYou need to do profiling focused on these components.\r\n\r\nBy the CPU and memory usage it does not seen to be a problem with your hardware sizing, but something related to the application configuration or component parametrization.\r\n\r\nI would start by checking how long does the SQL requests are taking. Just google `mysql database profiling tools` and you are gonna find your way to increase mySQL perfomance if needed.",
        "score": 21.515625,
        "rank": 17,
        "document_id": "c0bce809-4ad2-4f1a-9861-20e53ee66033",
        "passage_id": 341615
    },
    {
        "content": "&gt; I find experimentally that the RAM doesn&#39;t get its correct values until some clock cycles after power-up.\r\n\r\nUnfortunately you do not say how you do that. I&#39;m assuming you are using iCE40 synthesis and run it in hardware with SRAM programming, because that would match a known iCE40 hardware issue.\r\n\r\nSee also [here](https://github.com/cliffordwolf/icestorm/issues/76#issuecomment-289270411) and [here](http://svn.clifford.at/handicraft/2017/ice40bramdelay/) for more information.\r\n\r\nWorkarounds: Do not use SRAM programming or keep your design in reset a few more cycles to give BRAM initialization some time to complete.\r\n\r\nThe problem can also be reproduced when using the Lattice tools. It&#39;s a hardware bug. There is nothing the synthesis flow could do about it.\r\n\r\nYour HDL code is OK and should yield netlists with initialized memory resources when using a flow that supports initialized memories (such as iCE40 synthesis or Xilinx 7-series synthesis).\r\n\r\n----\r\n\r\nEdit re comment: You can ignore the warning re blocking vs nonblocking assignments in the initial block.",
        "score": 21.5,
        "rank": 18,
        "document_id": "26b5d041-10b0-447c-9c89-3a9a0bc4c45e",
        "passage_id": 295145
    },
    {
        "content": "From decades of experience with virtualization performance, this is an area to be cautious. A lot will depend on the level of contention involved between your virtual machine and others, which in many cloud environments, is difficult to know without tooling.\r\nAlso, it isn&#39;t clear whether you are discussing elapsed time and/or processor time. Both can be influenced by virtualization, though my experience is that elapsed time is more variable. \r\nI can&#39;t speak to the listed environments, but in IBM Z virtualization solutions, we provide metrics that cover processor time consumed by the virtual machine and that consumed by the hypervisor. For your purposes, you&#39;d want just that consumed by the virtual machine. Sorry, I don&#39;t know if either of the platforms you mentioned provide that information.\r\nIn these type of experiments, we often find it useful to do more measurement iterations to see run time variability.",
        "score": 21.484375,
        "rank": 19,
        "document_id": "eb17f36b-592d-4779-a7f6-411f9109b2ba",
        "passage_id": 262228
    },
    {
        "content": "Firstly, I wouldn&#39;t write my own load testing tool; there are plenty available. I&#39;ve used [JMeter][1] (open source). You can use JMeter (and other similar tools) to send both POST and GET [parameters][2], cookies and other HTTP headers - though admittedly, this does become challenging for complex cases.\r\n\r\nNext, make sure your problem really is the server, and not the other infrastructure - network, routers, firewalls etc. all have maximum capabilities, and may be the root cause of the problem. Most of them have logging and reporting tools. For instance, I&#39;ve seen tests report a throughput issue when they reached the maximum capacity of the firewall; the servers were not even close to breaking point. This happened because we had included a rather large binary file in the test cases, which normally would be served from a CDN. \r\n\r\nNext, on the whole it&#39;s unlikely that serving static HTTP requests is the problem - IIS is really, really [good][3] at that. For the kind of hardware you mention, I&#39;d expect to handle many thousands of requests per second. for static files.",
        "score": 21.453125,
        "rank": 20,
        "document_id": "452bbb0c-4585-4d6a-a194-2d9bc66835ae",
        "passage_id": 411561
    },
    {
        "content": "The Linux &quot;perf&quot; tool can use hardware performance counters to give you precise numbers for lots of things including instructions executed.\r\n\r\n    $ perf stat true\r\n    \r\n     Performance counter stats for &#39;true&#39;:\r\n    \r\n              0.183734 task-clock                #    0.314 CPUs utilized          \r\n                     0 context-switches          #    0.000 M/sec                  \r\n                     0 CPU-migrations            #    0.000 M/sec                  \r\n                   118 page-faults               #    0.642 M/sec                  \r\n               627,313 cycles                    #    3.414 GHz                    \r\n               396,604 stalled-cycles-frontend   #   63.22% frontend cycles idle   \r\n               268,222 stalled-cycles-backend    #   42.76% backend  cycles idle   \r\n               404,935 instructions              #    0.65  insns per cycle        \r\n                                                 #    0.98  stalled cycles per insn\r\n                75,949 branches                  #  413.364 M/sec                  \r\n                 3,602 branch-misses             #    4.74% of all branches        \r\n    \r\n           0.",
        "score": 21.421875,
        "rank": 21,
        "document_id": "9010e613-1ce0-47ae-b96d-ae54d5532c71",
        "passage_id": 103341
    },
    {
        "content": "It is a mistaken assumption that &quot;correct/proper/whatever your goodness metric is&quot; code will leak no memory.\r\n\r\nThere are two kinds of leaked memory: startup, or one-time allocations that won&#39;t ever be repeated, and ongoing leaks from allocations that can be repeated an arbitrary and ever-increasing number of times. The latter are understandably problematic and should be fought viciously until they perish.\r\n\r\nOn the contrary, **it is absolutely, no-qualms-about it wasteful** to free startup memory. Every CPU cycle devoted to it wasted when the very next thing to happen is process termination where all the memory is released in the most cycle- and energy-efficient way possible. These allocations aren&#39;t leaks. They are on *you* to add to your memory debug tool&#39;s exception list.\r\n\r\nThe cultish freeing of every single block of memory right before process termination has a measurable impact on energy efficiency of applications that are executed multiple times in rapid succession. For common short-lived processes such as many Unix core utilities and build tools, including compilers, the `free`s prior to exit have wasted many railcar-loads of coal in power plants all over the world, and this trend shows no signs of slowing down.",
        "score": 21.34375,
        "rank": 22,
        "document_id": "32c1d4f9-d59b-4ac4-823e-4ab930708af7",
        "passage_id": 321029
    },
    {
        "content": "On current processors and hardware (e.g. Intel or AMD or ARM in laptops or desktops or tablets) with common operating systems (Linux, Windows, FreeBSD, MacOSX, Android, iOS, ...) processes are [scheduled][1] at random times. So cache behavior is non deterministic. Hence, instruction timing is non reproducible. You *need* some hardware time measurement.\r\n\r\nA typical desktop or laptop gets hundreds, or thousands, of [interrupts][2] every second, most of them time related. Try running `cat /proc/interrupts` on a Linux machine twice, with a few seconds between the runs.\r\n\r\nI guess that even with a single-tasked MS-DOS like operating system, you&#39;ll still get random behavior (e.g. induced by ACPI, or [SMM][3]). On some laptops, the processor frequency can be throttled by its temperature, which depends upon the CPU load and the external temperature... \r\n\r\nIn practice you really want to use some timer provided by the operating system. For Linux, read [time(7)][4]\r\n\r\nSo **you practically *cannot* rely on a *purely software* timer**.",
        "score": 21.296875,
        "rank": 23,
        "document_id": "0ee6fcc6-46cf-4b30-88c6-a9e04d1292b2",
        "passage_id": 354106
    },
    {
        "content": "I think you have no problem with memory leak, I cannot see it from your jconsole chart at least. There is no major GC collection. So it seems only more and more objet allocated into tenured(old) generation. To ensure about memory leak you should Perform GC and after that compare allocated memory. If you find a leak you are able to make a heap dump with jmap or visual tool(standart JDK tools). After this heap dump can be analyzed with [MAT][1]. Before taking heap dump it is better to Perform GC to decrease heap dump file size.\r\n\r\nSome notes:\r\n\r\n* Threads count shouldn&#39;t affect heap memory explicitly. It maybe useful for you to review next [java memory structure][2]. So thread require stack memory not a heap.\r\n* In general it is not good idea to create cache for not heavy object, because of GC works algorithm.\r\n* Also I think you should consider [cached thread pool][3] or calibrate ThreadPoolSize according server hardware.",
        "score": 21.203125,
        "rank": 24,
        "document_id": "74352f24-0d98-41f3-9eec-d37f492829c2",
        "passage_id": 297761
    },
    {
        "content": "Do **not** use the big-O notation for the real life performance considerations.\r\n\r\nThat said, to the rest of the question:\r\n\r\nThe performance gathering will take some time (O(1) can still be meaningful time, it&#39;s just that it won&#39;t depend on your data). You need to make it the most efficient.\r\n\r\nThat means:\r\n\r\n1. Not to use `printf` and likes, but rather write to a special memory area, from which you&#39;ll extract the data later.\r\n\r\n2. For the same reason don&#39;t use `strcat`,  instead use `struct`s of binary data. Parse it in the end when you&#39;re done.\r\n\r\n3. Instead of measuring each call, consider measuring **averages** (i.e.: measure not each call, but each 1000 and average to extract the approximate cost of a single call). That will make your measurement overhead times lesser. That is not always a possibility though, but consider it.\r\n\r\n4. The `clock_gettime` can usually be trusted, but it depends on your OS and hardware - check them out, sometimes the hardware clock resolution might not be as small as you&#39;d wish.",
        "score": 21.203125,
        "rank": 25,
        "document_id": "dae973d2-d3f9-4761-bbfc-f6bdb4e2f8fa",
        "passage_id": 464616
    },
    {
        "content": "All these errors indicate that the behaviour you describe with your code cannot be implemented with hardware resources of your target.\r\n\r\n    ...because its behavior does not match any supported register model\r\n\r\nsays that the synthesis tool recognized a kind of register but not the kind it can implement.\r\n\r\n    ...because its behavior depends on the edges of multiple distinct clocks\r\n\r\ntells you a bit more: you described a multi-clocks register and this is not supported. As `Memory` is a local variable of your process, your problem comes from lines of the same process that you did not post. For example, if your code is surrounded by:\r\n\r\n    process(clk,inVALID)\r\n    begin\r\n      if rising_edge(clk) then\r\n        if falling_edge(inVALID) then   -- 14 signals will come\r\n          ... your code ...\r\n        end if;\r\n      end if;\r\n    end process;\r\n\r\n\r\nthen you have two clocks for your `Memory` register bank: `clk` and `inVALID` and each bit of `Memory` is supposed to be updated when there is a rising edge of `clk` and, simultaneously, a falling edge of `inVALID`.\r\n\r\nIt can also be some code before or after your code, still in the same process.",
        "score": 21.203125,
        "rank": 26,
        "document_id": "7e10ae63-5075-4c1d-b9c8-c27481c8e7c6",
        "passage_id": 351645
    },
    {
        "content": "For issue 1, in a simple, scalar pipeline, a stall introduces a pipeline bubble which cannot be &quot;popped&quot;. To allow an instruction later in program order to fill a pipeline bubble, that instruction would have to go past the stalled instruction. Supporting such reordering of instructions increases the complexity of the pipeline, which tends to increase design and production costs *and* to increase either pipeline depth or cycle time (as well as use more energy per active cycle [out-of-order execution can be more energy efficient in total even when more energy is used when active]). The mechanisms needed to support such reordering also increases the complexity of explaining pipelines.\r\n\r\nFor issue 2, with a more complex pipeline it is possible to begin execution of more than one instruction at the same time. Such processors are called superscalar. With in-order execution, only instructions in a consecutive sequence (in program order) can begin execution at the same time, and this requires that the instructions do not have data dependencies and that sufficient hardware resources are available to execute the instructions and handle their results.",
        "score": 21.1875,
        "rank": 27,
        "document_id": "f640270f-8d07-458d-be5b-55362b2a7df6",
        "passage_id": 408794
    },
    {
        "content": "4.  **Metric Definitions**: The tools may have different definitions or methods of measuring memory. For instance, some may account for more memory types than others in their measurement.\r\n\r\n    \r\n5.  **Time Zone Differences**: Even though your local computer is set to UTC, the tools might be using different time zone settings internally which could affect how the data is aligned and displayed.\r\n    \r\n\r\nTo resolve this issue:\r\n\r\n-   Ensure both metrics are intended to measure the same thing (e.g., total memory usage vs. working set).\r\n-   Check the documentation for both monitoring tools to understand exactly what they&#39;re measuring and how they aggregate data.\r\n\r\n\r\n\r\n**reference:**\r\n\r\nhttps://learn.microsoft.com/en-us/azure/container-apps/metrics\r\n\r\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/reference/supported-metrics/microsoft-app-containerapps-metrics",
        "score": 21.1875,
        "rank": 28,
        "document_id": "a0a09619-8134-4c02-9f40-5cf8e64b6819",
        "passage_id": 117827
    },
    {
        "content": "Then the top becomes OS / HW independent while the lower levels need to be platform dependent.\r\n\r\nCheck out the [PAPI APIs][1]. Note that the APIs appear to give you the world, but are really just an API set. Someone still has to implement what&#39;s on the other side of the API.\r\n\r\nNow if you can do your own special instrumentation, many (most?) motherboards and other hardware have measurement points (some undocumented) that provide thermal, current (and so power) information. This information is important for debugging devices and platforms.\r\n\r\n\r\n  [1]: http://icl.cs.utk.edu/papi/",
        "score": 21.171875,
        "rank": 29,
        "document_id": "92b2e902-3dee-4802-b6d3-921cd3e8cad1",
        "passage_id": 351593
    },
    {
        "content": "You aren&#39;t going to &quot;wear it out&quot; by running it when you don&#39;t need to.\r\n\r\nThe main problems are wasting power and RAM.\r\n\r\nIf you have enough of these, then the lesser problems are:\r\n\r\nThe wasted power will become heat, this may upset your temperature measurements (this is a very small amount though).\r\n\r\nHaving the DMA running will increase your interrupt latency and maybe also slow down the processor slightly, if it encounters bus contention (this only matters if you are close to capacity in these regards).\r\n\r\nHaving it running all the time may also have the advantage of more stable readings, not being perturbed turning things on and off.",
        "score": 21.171875,
        "rank": 30,
        "document_id": "719ac767-fe0e-4ce8-9792-36036da76a04",
        "passage_id": 160815
    },
    {
        "content": "It is likely based on this that the constant `kCLLocationAccuracyHundredMeters` does not typically require the GPS to be turned on, but you might reduce your requested accuracy to **ensure** the GPS if left off, and iOS will likely still deliver to you data that is accurate within the range you prefer.\r\n\r\n&gt; I also wonder how much is distanceFilter important to battery consumption?\r\n\r\nThis filter is involved more in how often updates are actually delivered to your app, the accuracy deals more with the hardware that will be required to get the fix.  A filter will reduce how often radios are turned on, but this is a larger power saving tool when GPS is involved.  If you are using the Significant-Change service, however, this part of the question is irrelevant.\r\n\r\n  [1]: http://developer.apple.com/library/ios/documentation/UserExperience/Conceptual/LocationAwarenessPG/CoreLocation/CoreLocation.html#//apple_ref/doc/uid/TP40009497-CH2-SW8",
        "score": 21.0625,
        "rank": 31,
        "document_id": "538b05d3-23b9-41bb-bf8d-362c83264538",
        "passage_id": 105070
    },
    {
        "content": "This microbenchmark can be simplified by [disabling hardware prefetchers][1] so that there is no need for randomization. It&#39;s recommended to use 1GB pages (or at least 2MB pages) instead of 4KB pages to ensure that the whole list is allocated from a contiguous chunk of physical memory. Otherwise, there is a chance that multiple 4KB pages may be mapped to the same cache sets, causing conflict misses.\r\n\r\nThe reason that pointer chasing works is that current Intel and AMD processors don&#39;t employ [value prediction][2] techniques.\r\n\r\nThere is another way to measure latency. You can use `RDTSC`/`RDTSCP` around a single memory access instruction, essentially treating a single memory access as a short elapsed-time event. See: https://stackoverflow.com/questions/52083481/memory-latency-measurement-with-time-stamp-counter.\r\n\r\n\r\n  [1]: https://software.intel.com/en-us/articles/disclosure-of-hw-prefetcher-control-on-some-intel-processors\r\n  [2]: https://en.wikipedia.org/wiki/Speculative_execution",
        "score": 21.0625,
        "rank": 32,
        "document_id": "5738be0f-2c8e-41f6-9bba-2aaae78cdc35",
        "passage_id": 239879
    },
    {
        "content": "I do not think it will change much the hardware cost: or gates are less complex than multiplexers but each slave will now have to zero its read data bus, which adds as many and gates. A good point is, maybe, a reduced activity and thus a lower power consumption: slaves that are not accessed by the master will keep their read data bus low. But as you synthesize all this with a logic synthesizer and place and route it with a CAD tool, I am almost sure that you will end up with the same results (area, power, frequency) as for the more classical second option.\r\n\r\nYour third option reminds me the principles of the daisy chain or the token ring. But as you want to avoid 3-states I doubt that it will bring any benefit in terms of hardware cost. If you pipeline it correctly (each slave samples the incoming master requests and processes them or passes them to the next) you will probably reach higher clock frequencies than with the classical bus, especially with a large number of slaves, but as, in average, a complete transaction will take more clock cycles, you will not improve the performance neither.",
        "score": 21.03125,
        "rank": 33,
        "document_id": "064f7881-3d86-4ef7-9736-6df7c83e71ea",
        "passage_id": 342651
    },
    {
        "content": "There are some great free tools available, mainly AMD&#39;s CodeAnalyst (from my experiences on my i7 vs my phenom II, its a bit handicapped on the Intel processor cause it doesn&#39;t have access to the direct hardware specific counters, though that might have been bad config). \r\n\r\nHowever, a lesser know tool is the [Intel Architecture Code Analyser][1] (which is free like CodeAnalyst), which is similar to the spu tool you described, as it details latency, throughput and port pressure (basically the request dispatches to the ALU&#39;s, MMU and the like) line by line for your programs assembly. Stan Melax gave a nice [talk][2] on it and x86 optimization at this years GDC, under the title &quot;hotspots, flops and uops: to-the-metal cpu optimization&quot;. \r\n\r\nIntel also has a few more tools in the same vein as IACA, avaibale under the performance tuning section of [their experimental/what-if code site][3], such as PTU, which is (or was) an experimental evolution of VTune, from what I can see, its free.",
        "score": 21.0,
        "rank": 34,
        "document_id": "d988dbe4-8231-42ea-b1fe-84b0305b50b7",
        "passage_id": 460518
    },
    {
        "content": "The value of a sampler indicates the texture image unit being accessed. Setting a sampler\u2019s value to `i` selects texture image unit number `i`. \r\n\r\n\r\n&lt;br/&gt;\r\nNote, the &quot;name&quot; of the texture object is generted by\r\n\r\n    glGenTextures(1, &amp;calibrationTexture_);\r\n\r\nThis &quot;name&quot; (an integral number) is hardware and driver dependent.&lt;br/&gt;\r\nBut since there is only 1 texture objects generated in your code, there is a good chance that the value of the name is 1.  \r\n\r\nThis means, when you set\r\n\r\n    gl_-&gt;glActiveTexture(GL_TEXTURE1);\r\n\r\nyour code works, because the value of `calibrationTexture_` is 1.",
        "score": 20.921875,
        "rank": 35,
        "document_id": "c114e8cc-2885-4bf0-859b-fdaf78113fe2",
        "passage_id": 46576
    },
    {
        "content": "You have many questions here that might be better suited as individual questions or asked on [the Neo4j slack channel](http://neo4j.com/slack).\r\n\r\nI started to write this as a comment, but ran out of chars so I&#39;ll try to point you to some resources:\r\n\r\n**1) Neo4j distributed graph model**\r\n\r\nI&#39;m not sure exactly what you&#39;re asking here. See [this document](http://info.neo4j.com/rs/neotechnology/images/Understanding%20Neo4j%20Scalability(2).pdf) for general information on Neo4j scalability. If you&#39;re asking if graph traversals can be distributed across machines in a Neo4j cluster, the answer is no.\r\n\r\n**2) Hardware sizing**\r\n\r\nThis depends a bit on your access patterns. See the [hardware sizing calculator](https://neo4j.com/hardware-sizing-calculator) that can take workload / access patterns into account.\r\n\r\n**3-4) Import**\r\n\r\nCan you create a new question and share your code for this? You should be able to achieve much better performance than this.",
        "score": 20.921875,
        "rank": 36,
        "document_id": "ad7e704b-4831-4038-bf4c-4a22ce949a08",
        "passage_id": 323773
    },
    {
        "content": "**To use or not to use**\r\n\r\nIt is advised to use hardware acceleration only if you have complex custom computations for scaling, rotating and translating of images, but do not use it for drawing lines or curves (and other trivial operations) ([source][1]).\r\n\r\nIf you plan on having common transitions and also given that you have already considered scaling, recycling, caching etc, than it may not make sense to burden your project anymore. Also, any efforts spent reworking your code to support hardware acceleration will not effect users on versions below 3.0, which are ~36% of the market as of May 8, 2013.\r\n\r\n**Memory**\r\n\r\nRegarding memory usage (according to [this article][2]), by including Android Hardware the application loads up the OpenGL drivers for each process, takes memory usage of roughly 2MB, and boosts it to 8MB.\r\n\r\n**Other issues**\r\n\r\nApart from API versions, I presume it will also affect battery life. Unfortunately there aren&#39;t any benchmarks on different use cases online in order to draw a line on this one. Some argue that in given cases because of multiple gpu cores, using acceleration may save battery life.",
        "score": 20.921875,
        "rank": 37,
        "document_id": "13f9f17e-d2a4-49a2-817a-bc9e9ec3edfe",
        "passage_id": 292385
    },
    {
        "content": "It appears to me that you want to measure the number of memory accesses per rank in order to estimate the the per-rank energy from the total DRAM energy, but this is not trivial at all for the following reasons:\r\n\r\n - Not all CAS commands are of the same energy cost. Precharge and activate commands are not counted by any event and may consume significant energy, especially with high row buffer miss rates.\r\n - Even if there are zero requests in the IMC, as long as there is at least one active core, the memory channels are powered and do consume energy.\r\n - The amount of time it takes to process a request of the same type and to the same address may vary depending surrounding requests due to timing delays required by rank-to-rank turnarounds and read-write switching.\r\n\r\nDespite of all of that, I imagine it may be possible to build a good model of upper and lower bounds on per-rank energy given a representative estimate of the number of requests to each rank (as discussed above).\r\n\r\nThe bottom line is that there is no easy way to get the luxury of per-rank counting like on server processors.",
        "score": 20.921875,
        "rank": 38,
        "document_id": "edae2328-33fd-44a7-aedd-7f20465becfa",
        "passage_id": 185654
    },
    {
        "content": "Your measurements seem to be directed at determining the *relative* power consumption when running with 400 different variants of the code. It does not seem critical that steady-state power consumption is achieved, just that the conditions under which each variant is tested are as equal as is practically achievable. Keep in mind that the GPU&#39;s power sensors are not designed to provide high-precision measurements, so for comparison purposes you would want to assume a noise level on the order of 5%. For an accurate comparison you may even want to average measurements from more than one GPU of the same type, as manufacturing tolerances could cause variations in power draw between multiple &quot;identical&quot; GPUs.\r\n\r\nI would therefore suggest the following protocol: Run each variant for 30 seconds, measuring power consumption close to the end of that interval. Then let the GPU idle for 30 seconds to let it cool down before running the next kernel. This should give roughly equal starting conditions for each variant. You may need to lengthen the proposed idle time a bit if you find that the temperature stays elevated for a longer time. The temperature data reported by `nvidia-smi` can guide you here.",
        "score": 20.859375,
        "rank": 39,
        "document_id": "6f47dae6-71b1-4936-b66e-892d43ed3358",
        "passage_id": 11806
    },
    {
        "content": "---\r\n**EDIT**:\r\n\r\nWell reading your updates (which appeared between my starting of writing this answer and its publication), it looks like I was somewhat right (for the Windows and the Android). However, I was just wrong for `clock()`., but still, using a common timer like `omp_get_wtime()` on both platforms would be a good idea.\r\n\r\nNevertheless, I don&#39;t believe any of what I proposed so far can explain the discrepancy between the speed-ups seen on both machines. I suspect that the bottom line might just be the hardware characteristics. Again, this is very speculative (especially considering I never tried to run anything on a smartphone), but this could just be the consequence of (very) different balances between memory bandwidth, cache sizes and CPU performances on both machines:\r\n\r\n - The speed-up you get on PC (~5x for 8 cores) is good but not perfect. This means that you&#39;re already reaching a bottleneck there, which is likely either the memory bandwidth, or the cache size. This means that your code is likely to be sensitive to these hardware parameters.\r\n - The speed-up measured on the smartphone show very little improvement over the sequential code.",
        "score": 20.828125,
        "rank": 40,
        "document_id": "3bb27b4e-15c4-4438-9392-8a30d8e20e05",
        "passage_id": 344870
    },
    {
        "content": "You can use [valgrind][1] to find memory leaks or profiling. And [Google benchmark][2] for code snippet benchmarks. Or you can measure time yourself with [chrono][3].\r\nAnd an online benchmark tool, [quick-bench][4]\r\n\r\n**Update:**\r\n\r\nI found this nice info on [godbolt][5] which gives insights on the number of iterations or cycles...\n\nThanks to @[Peter][6] the [tool][7] \rllvm-mca&#160;estimates the Instructions Per Cycle (IPC), as well as hardware resource pressure.\r\nWhich is a simulation on a theoretical model of the CPU, not a profile but still could be useful. It also does not cover cache miss.\r\n\r\r\n\r\n\n\n  [1]: http://www.valgrind.org/\n  [2]: https://github.com/google/benchmark\n  [3]: https://en.cppreference.com/w/cpp/chrono\n  [4]: http://quick-bench.com/\n  [5]: https://godbolt.org/z/86JgA6\n  [6]: https://stackoverflow.com/users/224132/peter-cordes\n  [7]: https://llvm.org/docs/CommandGuide/llvm-mca.html",
        "score": 20.78125,
        "rank": 41,
        "document_id": "6af86fc4-dbae-4ef3-9029-0a8170ec7ace",
        "passage_id": 235421
    },
    {
        "content": "The increase in register use when switching from a double-precision multiplication to a double-precision division in kernel computation is due to the fact that double-precision multiplication is a built-in hardware instruction, while double-precision division is a sizable called software subroutine (that is, a function call of sorts). This is easily verified by inspection of the generated machine code (SASS) with `cuobjdump --dump-sass`.\r\n\r\nThe reason that double-precision divisions (and in fact all divisions, including single-precision division and integer division) are emulated either by inline code or called subroutines is due to the fact that the GPU hardware has no direct support for division operations, in order to keep the individual computational cores (&quot;CUDA cores&quot;) as simple and as small as possible, which ultimately leads to higher peak performance for a given size chip. It likely also improves the efficiency of the cores as measured by the GFLOPS/watt metric.\r\n\r\nFor release builds, the typical increase in register use caused by the introduction of double-precision division is around 26 registers. These additional registers are needed to store intermediate variables in the division computation, where each double-precision temporary variable requires two 32-bit registers.",
        "score": 20.734375,
        "rank": 42,
        "document_id": "ace9ec86-49a7-4890-9b48-43902012b06d",
        "passage_id": 20101
    },
    {
        "content": "In the GPGPU/CUDA community it is indeed known that different hardware platforms and CUDA library versions might yield different results when using the same API. The differences are always small. So, there is some heterogeneity across platforms.\r\n\r\nIndeed, this makes it tedious to write tests based on numerical results. The classification of *right* and *wrong* becomes less sharp and one must answer &quot;what is good enough?&quot;. One might consider this crazy and in many cases problematic, or even faulty. I think this should not be disputed here.\r\n\r\nWhat do you think, where did the `2e-6` tolerance come from in the first place? I&#39;d say someone *tried* to find a trade-off between how much of a variance he/she thought is sufficiently correct and how much of a variance he/she *expected*, practically. In the CPU world `2e-6` is already *large*. Hence, here someone chose a *large* tolerance in order to account for an expected degree of heterogeneity among GPU platforms.\r\n\r\nIn this case, practically, it probably means that the tolerance has not been chosen to reflect the real-world heterogeneity of GPU platforms.",
        "score": 20.71875,
        "rank": 43,
        "document_id": "8c599a20-8f33-416c-9a38-7af9bb7d1251",
        "passage_id": 22929
    },
    {
        "content": "There is something different about your application in the Production and UAT environments.\r\n\r\nJudging from the symptoms, it is (IMO) unlikely to be a hardware, operating system performance tuning or a difference in the JVM versions.  It goes without saying that this is unlikely to be due to the application having more memory.  \r\n\r\n(It is not inconceivable  that your application might do something strange ... like sizing some data structures based on the maximum heap size and get the calculations wrong.  But I think you&#39;d be aware of that possibility, so lets ignore it for now.)\r\n\r\nIt is probably *related to* a difference in the OS environment; e.g. a different version of the OS or some application, differences in the networking, differences in locales, etcetera.  But the bottom line is that it is 99% certain that there is a memory leak in your application when run on the UAT, and that memory leak is what is chewing up heap memory and overloading the GC.\r\n\r\nMy advice would be to treat this as a storage leak problem, and use the standard tools / techniques to track down the cause of the problem.  In the process, you will most likely be able to figure out why this only occurs on your UAT.",
        "score": 20.703125,
        "rank": 44,
        "document_id": "0ab1a1a0-5023-4729-b895-fa2eff947f92",
        "passage_id": 439493
    },
    {
        "content": "In reverse order:\r\n\r\n(3). You need your own memory management routines, if not for any other reason, then simply because you expect that it may be ported to a bare platform without an underlying OS. Then you also need an implementation for pretty much everything else: string libraries, math libraries etc. You have to either code them yourself, or try to find ready-made code libraries that provide them for each platform.\r\n\r\n(2). The OS and CPU are somewhat orthogonal variables. You would probably have a better time creating two abstraction layers (one for different operating systems, one for different hardware platforms) and then include/override definitions as necessary for each new platform. But yes, an abstraction layer is a more manageable solution than riddling your code with whole hordes of #ifdefs.\r\n\r\n(1). This is not an easy question to answer. For example if you expect your library to run on an embedded system or even a microcontroller then it&#39;s quite probable that not all of ANSI C features are available, depending on the maturity of the development tools. There could also be restrictions not necessarily related to the language itself. Hardware floating point units are relatively rare in embedded systems.",
        "score": 20.703125,
        "rank": 45,
        "document_id": "feea1e0b-a01a-47ea-b7a7-81fb7362b8e3",
        "passage_id": 473907
    },
    {
        "content": "I think the problem might be with the measurement method. Timing an individual GL command or even a render is very difficult because the driver will be trying to keep all stages of the GPU&#39;s pipeline busy by running different parts of multiple renders in parallel. For this reason the driver is probably ignoring glFinish and will only wait for the hardware to finish if it must (e.g. glReadPixels on a render target).\r\n\r\nIndividual renders might appear to complete very quickly if the driver is just adding them to the end of a queue but very slowly if it needs to wait for space in the queue and has to wait for an earlier render to finish.\r\n\r\nA better method would be to run a large number of frames (e.g. 1000) and measure the total time for all of them.",
        "score": 20.6875,
        "rank": 46,
        "document_id": "b30bba7b-6226-4e5d-a542-962b354aaef5",
        "passage_id": 109494
    },
    {
        "content": "Usually, modern GPUs are working with images very well, because originally they were designed to do so. Unless you are using some specific hardware, which isn&#39;t good with Image data structure, such refactoring is a good idea (IMO).\r\n\r\nChanging the amount of data, that every WI or WG process, doesn&#39;t mandatory lead to performance decrease - algorithm changes may influence performance in unpredictable way. So, to make conclusions, careful benchmarks are needed.\r\n\r\nIf you are targeting specific hardware, using SDK tools is always a good idea. Performance analyzers can give useful statistics &amp; optimization hints (such as register spilling, cache hit rate, utilization percentage, etc.)",
        "score": 20.640625,
        "rank": 47,
        "document_id": "2a59ad52-50da-42b7-a841-cef800cb03b8",
        "passage_id": 384153
    },
    {
        "content": "The memory usage metrics of the same Azure container app are shown in two screenshots, but they have different maximum values, which creates confusion.\r\n\r\nHere are the reasons for the discrepancy:\r\n\r\n1.  **Different Metrics**: A possible reason for the discrepancy between the two screenshots is that they are measuring different aspects of memory usage. The first screenshot could be displaying the average value of a metric that accumulates data over time, while the second screenshot could be showing the peak value of the working set bytes.\r\n\r\n    \r\n2.  **Sampling Intervals**: A possible reason for the discrepancy in the tools&#39; results is that they sample the memory usage at different intervals. This can affect how they capture the variations in memory consumption over time.\r\n.\r\n    \r\n3.  **Aggregation Over Time**: One possible tool is to aggregate data over a longer period, such as an hour&#39;s average, and another is to display data that is more current or has a shorter aggregation time.\r\n    \r\n4.  **Metric Definitions**: The tools may have different definitions or methods of measuring memory. For instance, some may account for more memory types than others in their measurement.\r\n\r\n    \r\n5.",
        "score": 20.640625,
        "rank": 48,
        "document_id": "a0a09619-8134-4c02-9f40-5cf8e64b6819",
        "passage_id": 117826
    },
    {
        "content": "(Hardware Lock Elision is a very constrained implementation of silent ABA store elimination. It has the implementation advantage that the check for value consistency is explicitly requested.)\r\n\r\nThere are also implementation issues in terms of performance impact/design complexity. Such would prohibit avoiding read-for-ownership (unless the silent store elimination was only active when the cache line was already present in shared state), though read-for-ownership avoidance is also currently not implemented.\r\n\r\nSpecial handling for silent stores would also complicate implementation of a memory consistency model (probably especially x86&#39;s relatively strong model). Such might also increase the frequency of rollbacks on speculation that failed consistency. If silent stores were only supported for L1-present lines, the time window would be very small and rollbacks *extremely* rare; stores to cache lines in L3 or memory might increase the frequency to very rare, which might make it a noticeable issue.\r\n\r\nSilence at cache line granularity is also less common than silence at the access level, so the number of invalidations avoided would be smaller.\r\n\r\nThe additional cache bandwidth would also be an issue. Currently Intel uses parity only on L1 caches to avoid the need for read-modify-write on small writes.",
        "score": 20.625,
        "rank": 49,
        "document_id": "590a08d8-994d-4b86-8912-115df40b5131",
        "passage_id": 46992
    },
    {
        "content": "There are at least 2 reasons for using sequential logic: 1) logic related and 2) hardware related.\r\n\r\nThe first just depends on the needs of your design and can be used to implement memories, synchronize between different parts of logic, or break combinational cycles.  In your small example there is no logical need for such an action.\r\n\r\nThe second is due to the electrical delays in modern hardware. You can imagine a logical graph. Electrical signals go different paths and have different delays over the paths. As a result, the correct electrical answer will be settled after the longest path gets settled. In the meanwhile the output signals will just jiggle, consuming power. So, you need to know how long it takes to settle down. Till then you&#39;d like to keep the old values in the outputs and only update them when the result is ready. In order to preserve power and make logic work smoothly you need to break your design into smaller combination pieces and synchronize their execution with some clock.\r\n\r\nSince you are using a lot of addition/subtraction in your design, your logic could be too big for real hardware to execute as a single chunk. Synthesis flow tools will tell you about it.",
        "score": 20.578125,
        "rank": 50,
        "document_id": "df248750-9961-41ce-a74f-c48bc9f900f4",
        "passage_id": 216342
    }
]